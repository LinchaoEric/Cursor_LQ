- [Technical Skills](#technical-skills)
  - [What can you expect?](#what-can-you-expect)
  - [How to Prep](#how-to-prep)
    - [Aspects of the Technical Skills questions include:](#aspects-of-the-technical-skills-questions-include)
    - [Additional Tips:](#additional-tips)
- [SQL Questions for Dating App's Profile Reporting System](#sql-questions-for-dating-apps-profile-reporting-system)
  - [Table Schemas](#table-schemas)
    - [`all_profiles`](#all_profiles)
    - [`reported_profiles`](#reported_profiles)
    - [Sample Data](#sample-data)
  - [Questions \& Solutions](#questions--solutions)
    - [Q1: Consistency Rate of User Reports vs Final Decisions](#q1-consistency-rate-of-user-reports-vs-final-decisions)
    - [Q2: Most Common Violation Type](#q2-most-common-violation-type)
    - [Q3: Time to First Report Analysis](#q3-time-to-first-report-analysis)
    - [Q4: Identifying Report Tool Abuse](#q4-identifying-report-tool-abuse)
  - [Key Considerations](#key-considerations)
- [Message Conversations](#message-conversations)
  - [Table Structure](#table-structure)
  - [Interview Questions and Solutions](#interview-questions-and-solutions)
    - [1. Number of Unique Conversations in Last Week](#1-number-of-unique-conversations-in-last-week)
    - [2. Percentage of Conversations with Reactions](#2-percentage-of-conversations-with-reactions)
    - [3. Average Time to First Reaction](#3-average-time-to-first-reaction)
    - [4. Active Conversations Analysis](#4-active-conversations-analysis)
  - [Key Considerations](#key-considerations-1)
  - [Common Follow-up Questions](#common-follow-up-questions)
- [App Usage Analysis](#app-usage-analysis)
  - [Table Structure](#table-structure-1)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-1)
    - [1. Most Used App in Last 30 Days](#1-most-used-app-in-last-30-days)
    - [2. Time Spent Percentage by App Category](#2-time-spent-percentage-by-app-category)
    - [3. Category Engagement Analysis](#3-category-engagement-analysis)
    - [4. Retention Analysis by Category](#4-retention-analysis-by-category)
  - [Key Considerations](#key-considerations-2)
  - [Common Follow-up Questions](#common-follow-up-questions-1)
  - [Performance Optimization Tips](#performance-optimization-tips)
- [Ads Performance and Revenue Analysis](#ads-performance-and-revenue-analysis)
  - [Table Structure](#table-structure-2)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-2)
    - [1. Global Ads Revenue Analysis](#1-global-ads-revenue-analysis)
    - [2. Best Performing Ads](#2-best-performing-ads)
    - [3. Next Ad Recommendation](#3-next-ad-recommendation)
  - [Key Considerations](#key-considerations-3)
  - [Follow-up Questions](#follow-up-questions)
  - [Performance Optimization Tips](#performance-optimization-tips-1)
- [Search Quality Measurement](#search-quality-measurement)
  - [Table Structure](#table-structure-3)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-3)
    - [1. Basic Search Quality Score](#1-basic-search-quality-score)
    - [2. Advanced Quality Metrics](#2-advanced-quality-metrics)
    - [3. Query-Specific Performance Analysis](#3-query-specific-performance-analysis)
  - [Key Considerations](#key-considerations-4)
  - [Follow-up Questions](#follow-up-questions-1)
  - [Performance Optimization Tips](#performance-optimization-tips-2)
- [User Activity Analysis](#user-activity-analysis)
  - [Table Structure](#table-structure-4)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-4)
    - [1. Define and Compare New vs. Existing Users](#1-define-and-compare-new-vs-existing-users)
    - [2. Detailed Activity Analysis](#2-detailed-activity-analysis)
    - [3. Retention Analysis by User Type](#3-retention-analysis-by-user-type)
  - [Key Considerations](#key-considerations-5)
  - [Follow-up Questions](#follow-up-questions-2)
  - [Performance Optimization Tips](#performance-optimization-tips-3)
- [Comment Distribution](#comment-distribution)
  - [Table Structure](#table-structure-5)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-5)
    - [1. Basic Comment Distribution Metrics](#1-basic-comment-distribution-metrics)
    - [2. Comment Distribution by Content Type](#2-comment-distribution-by-content-type)
    - [3. Time-Based Comment Distribution](#3-time-based-comment-distribution)
    - [4. Comment Depth Analysis](#4-comment-depth-analysis)
  - [Key Considerations](#key-considerations-6)
  - [Follow-up Questions](#follow-up-questions-3)
  - [Performance Optimization Tips](#performance-optimization-tips-4)
- [Spam and Fraud Detection Analysis](#spam-and-fraud-detection-analysis)
  - [Table Structure](#table-structure-6)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-6)
    - [1. Identify Potentially Fraudulent Accounts](#1-identify-potentially-fraudulent-accounts)
    - [2. Detect Ad Click Fraud](#2-detect-ad-click-fraud)
    - [3. Multiple Account Detection](#3-multiple-account-detection)
    - [4. Report Analysis for Dating App](#4-report-analysis-for-dating-app)
  - [Key Considerations](#key-considerations-7)
  - [Follow-up Questions](#follow-up-questions-4)
  - [Performance Optimization Tips](#performance-optimization-tips-5)
- [Survey Data](#survey-data)
  - [Table Structure](#table-structure-7)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-7)
    - [1. Basic Response Rate Analysis](#1-basic-response-rate-analysis)
    - [2. Response Rate by User Segment](#2-response-rate-by-user-segment)
    - [3. Response Time Analysis](#3-response-time-analysis)
    - [4. Response Quality Analysis](#4-response-quality-analysis)
  - [Key Considerations](#key-considerations-8)
  - [Follow-up Questions](#follow-up-questions-5)
  - [Performance Optimization Tips](#performance-optimization-tips-6)
- [Friending](#friending)
  - [Table Structure](#table-structure-8)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-8)
    - [1. Daily Acceptance Analysis for Past 4 Weeks](#1-daily-acceptance-analysis-for-past-4-weeks)
    - [2. Friend Request Analysis by Age Group](#2-friend-request-analysis-by-age-group)
    - [3. Friend Request Success Rate Analysis](#3-friend-request-success-rate-analysis)
    - [4. Network Growth Analysis](#4-network-growth-analysis)
  - [Key Considerations](#key-considerations-9)
  - [Common Follow-up Questions](#common-follow-up-questions-2)
  - [Performance Optimization Tips](#performance-optimization-tips-7)
- [Session Analysis - Meta SQL Interview Guide](#session-analysis---meta-sql-interview-guide)
  - [Table Structure](#table-structure-9)
  - [Interview Questions and Solutions](#interview-questions-and-solutions-9)
    - [1. Time Between Sessions Analysis](#1-time-between-sessions-analysis)
    - [2. Bounce Rate Analysis (A-\>B-\>A Pattern)](#2-bounce-rate-analysis-a-b-a-pattern)
    - [3. Detailed Bounce Analysis by User Segments](#3-detailed-bounce-analysis-by-user-segments)
    - [4. Session Quality Metrics](#4-session-quality-metrics)
  - [Key Considerations](#key-considerations-10)
  - [Follow-up Questions](#follow-up-questions-6)
  - [Performance Optimization Tips](#performance-optimization-tips-8)
- [User Behavior Analysis - Meta SQL Interview Guide](#user-behavior-analysis---meta-sql-interview-guide)
  - [1. Phone Number Confirmation Analysis](#1-phone-number-confirmation-analysis)
    - [Tables](#tables)
    - [Questions \& Solutions](#questions--solutions-1)
  - [2. App Switching Analysis](#2-app-switching-analysis)
    - [Tables](#tables-1)
    - [Questions \& Solutions](#questions--solutions-2)
  - [3. User Status Tracking](#3-user-status-tracking)
    - [Tables](#tables-2)
    - [Questions \& Solutions](#questions--solutions-3)
  - [Key Considerations](#key-considerations-11)
  - [Follow-up Questions](#follow-up-questions-7)
  - [Performance Optimization Tips](#performance-optimization-tips-9)


# Technical Skills

## What can you expect?
In your Technical Skills interview, your interviewer will assess your performance on 4 focus areas:

- **Programming**: Develops a solution to complex data problems using programming and scripting languages.
- **Communicating Effectively**: Provides timely, clear, concise information to others and adjusts communications to be appropriate for the audiences.
- **Data Analysis**: Leverages methods ranging from descriptive stats to measurement models to answer exploratory and hypothesis-based questions to inform data-driven decisions.
- **Determining Goals & Success Metrics**: Identifies metrics that reflect operational success and inform business objectives.

## How to Prep
Interviewers can only assess your skills and abilities based on what you show them during your interview, so it’s important to plan and prepare to best showcase your strengths. 

### Aspects of the Technical Skills questions include:
- Structuring and articulating a solution based on data when presented with an open-ended problem.
- Coding an executable solution based on the articulated approach.
- Identifying and addressing edge cases.
- Adapting or changing code based on new information and/or constraints.
- Mindfulness of code efficiency.

### Additional Tips:
- You will **NOT** be assessed on the ability to code in a specific language or flavor of a language (e.g., any type of SQL is acceptable).
- Most questions are designed with SQL in mind, but you should feel free to execute in your language of choice. Please let the recruiter know your preference.
- You will be discouraged from using any convenient “magic” functions that trivially solve the problem.
- While some syntax details that can be easily googled might be forgiven, pseudo-code that allows you to gloss over details is **not** acceptable.
- Interviewers generally know the most widely used functions or syntax, but there’s no guarantee that any individual interviewer will be familiar with the ones you’re mentioning. You may have to provide more guidance/context around what you’re writing.
- Ads revenue - rolling sum window function, use YoY% to forecast next year   

Let me help you create a well-structured markdown response for SQL questions related to a dating app's profile reporting system.

# SQL Questions for Dating App's Profile Reporting System

## Table Schemas

### `all_profiles`
```sql
-- Contains all profiles created on the app
creation_date STRING '2017-12-01'  -- Date profile was created
profile_id BIGINT                  -- Unique identifier of the profile
```

### `reported_profiles`
```sql
date STRING '2017-12-01'           -- Date of report
profile_id BIGINT                  -- ID of reported profile  
reporter_id BIGINT                 -- ID of user making report
reported_policy STRING             -- Type of violation reported
final_policy STRING               -- Final decision after review ("None" if no violation)
```

### Sample Data
```sql
date       | profile_id | reporter_id | reported_policy       | final_policy
-----------|-----------|-------------|----------------------|---------------
2020-03-01 | 1000      | 1           | Fake picture         | Fake picture
2020-03-01 | 1000      | 1           | Impersonation        | Impersonation  
2020-03-01 | 1003      | 2           | Sexually Inappropriate| Sexually Inappropriate
2020-03-01 | 1005      | 3           | Fake picture         | None
```

## Questions & Solutions

### Q1: Consistency Rate of User Reports vs Final Decisions
Calculate how often user-reported violations match the final decision.

```sql
SELECT 
  SUM(CASE WHEN reported_policy = final_policy THEN 1 ELSE 0 END) * 1.0 / 
  COUNT(*) as consistency_rate
FROM reported_profiles;
```

### Q2: Most Common Violation Type
Find which policy has the highest number of confirmed violations.

```sql
SELECT 
  final_policy,
  COUNT(*) as violation_count
FROM reported_profiles
WHERE final_policy != 'None'
GROUP BY final_policy
ORDER BY violation_count DESC
LIMIT 1;
```

### Q3: Time to First Report Analysis 
Calculate average time between profile creation and first report.

```sql
WITH profile_first_report AS (
  SELECT 
    profile_id,
    MIN(date) as first_report_ts
  FROM reported_profiles
  WHERE profile_id IN (
    SELECT profile_id
    FROM reported_profiles
    WHERE final_policy IS NOT NULL
  )
  GROUP BY profile_id
)
SELECT 
  AVG(DATE_DIFF('day', date(b.creation_date), a.first_report_ts)) as avg_elapsed_days
FROM profile_first_report a
LEFT JOIN all_profiles b
  ON a.profile_id = b.profile_id;
```

### Q4: Identifying Report Tool Abuse
Detect users who may be creating false positive reports.

```sql
WITH reporter_stat AS (
  SELECT 
    reporter_id,
    COUNT(reported_policy) as reported_num,
    SUM(CASE WHEN final_policy IS NULL THEN 1 ELSE 0 END) as final_suitable_num,
    SUM(CASE WHEN final_policy IS NULL THEN 1 ELSE 0 END) * 1.0 / 
      COUNT(reported_policy) as report_fail_rate
  FROM reported_profiles
  GROUP BY reporter_id
),
threshold AS (
  SELECT 
    APPROX_PERCENTILE(reported_num, 0.95) as report_num_p95,
    APPROX_PERCENTILE(report_fail_rate, 0.95) as report_fail_rate_p95,
    AVG(reported_num) + 2 * STDDEV(reported_num) as reported_num_threshold,
    AVG(report_fail_rate) + 2 * STDDEV(report_fail_rate) as report_fail_rate_threshold
  FROM reporter_stat
)
SELECT reporter_id
FROM reporter_stat r
CROSS JOIN threshold t
WHERE 
  reported_num > report_num_p95 
  AND report_fail_rate > report_fail_rate_p95;
```

## Key Considerations

1. When calculating consistency rates, consider:
   - Multiple reports on same profile
   - Different violation types
   - Time window between reports

2. For abuse detection:
   - Look at volume and frequency of reports
   - False positive rates compared to population
   - Report patterns over time

3. General best practices:
   - Use appropriate date/time functions for your SQL dialect
   - Consider performance with large datasets
   - Handle NULL values appropriately

# Message Conversations

## Table Structure
```sql
Table: messages
date       | timestamp           | sender_id | receiver_id | msg_id | has_reaction
--------------------------------------------------------------------------------
2024-01-01 | 2024-01-01 10:00:00| 101       | 201         | 1      | FALSE
2024-01-01 | 2024-01-01 10:05:00| 201       | 101         | 2      | TRUE
2024-01-01 | 2024-01-01 10:10:00| 101       | 201         | 3      | FALSE
2024-01-01 | 2024-01-01 11:00:00| 102       | 202         | 4      | FALSE
2024-01-01 | 2024-01-01 11:05:00| 202       | 102         | 5      | TRUE
2024-01-02 | 2024-01-02 09:00:00| 103       | 203         | 6      | FALSE
2024-01-02 | 2024-01-02 09:05:00| 203       | 103         | 7      | FALSE
2024-01-02 | 2024-01-02 10:00:00| 101       | 201         | 8      | TRUE
2024-01-02 | 2024-01-02 10:30:00| 104       | 204         | 9      | FALSE
2024-01-02 | 2024-01-02 11:00:00| 102       | 202         | 10     | TRUE
```

## Interview Questions and Solutions

### 1. Number of Unique Conversations in Last Week

**Question:** How many unique conversations (threads) occurred in the last week?

**Clarification Points:**
- Define what constitutes a unique conversation
- Handling sender-receiver pairs (e.g., 101-201 and 201-101 are same conversation)
- Time window definition

**Solution:**
```sql
WITH conversations AS (
    SELECT 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END as user1,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END as user2,
        DATE(timestamp) as conv_date
    FROM messages
    WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '7' DAY
)
SELECT COUNT(DISTINCT CONCAT(user1, '-', user2)) as unique_conversations
FROM conversations;
```

### 2. Percentage of Conversations with Reactions

**Question:** What percentage of conversations have at least one reaction?

**Clarification Points:**
- Definition of conversation (entire history vs. specific time period)
- Whether to count multiple reactions in same conversation
- Treatment of NULL values

**Solution:**
```sql
WITH conversation_stats AS (
    SELECT 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END as user1,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END as user2,
        MAX(CASE WHEN has_reaction THEN 1 ELSE 0 END) as has_any_reaction
    FROM messages
    GROUP BY 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END
)
SELECT 
    ROUND(
        COUNT(CASE WHEN has_any_reaction = 1 THEN 1 END) * 100.0 / 
        NULLIF(COUNT(*), 0),
        2
    ) as pct_conversations_with_reactions
FROM conversation_stats;
```

### 3. Average Time to First Reaction

**Question:** What is the average time (in days) from conversation start to first reaction?

**Clarification Points:**
- Definition of conversation start (first message between pair)
- Handling conversations with no reactions
- Time unit for results (days, hours, minutes)

**Solution:**
```sql
WITH conversation_times AS (
    SELECT 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END as user1,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END as user2,
        MIN(timestamp) as first_message,
        MIN(CASE WHEN has_reaction THEN timestamp END) as first_reaction
    FROM messages
    GROUP BY 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END
    HAVING MIN(CASE WHEN has_reaction THEN timestamp END) IS NOT NULL
)
SELECT 
    AVG(EXTRACT(EPOCH FROM (first_reaction - first_message))/86400.0) as avg_days_to_reaction
FROM conversation_times;
```

### 4. Active Conversations Analysis

**Question:** Prove that conversations with reactions are more active than those without reactions.

**Clarification Points:**
- Definition of "active" (message frequency, response time, etc.)
- Time period for analysis
- Metrics to compare

**Solution:**
```sql
WITH conversation_metrics AS (
    SELECT 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END as user1,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END as user2,
        COUNT(*) as message_count,
        COUNT(DISTINCT DATE(timestamp)) as active_days,
        MAX(CASE WHEN has_reaction THEN 1 ELSE 0 END) as has_reaction,
        AVG(EXTRACT(EPOCH FROM (
            timestamp - LAG(timestamp) OVER (
                PARTITION BY 
                    LEAST(sender_id, receiver_id), 
                    GREATEST(sender_id, receiver_id)
                ORDER BY timestamp
            )
        ))/3600.0) as avg_response_time_hours
    FROM messages
    GROUP BY 
        CASE 
            WHEN sender_id < receiver_id THEN sender_id 
            ELSE receiver_id 
        END,
        CASE 
            WHEN sender_id < receiver_id THEN receiver_id 
            ELSE sender_id 
        END
)
SELECT 
    has_reaction,
    AVG(message_count) as avg_messages,
    AVG(active_days) as avg_active_days,
    AVG(avg_response_time_hours) as avg_response_time
FROM conversation_metrics
GROUP BY has_reaction
ORDER BY has_reaction;
```

## Key Considerations

1. **Data Quality**
   - Handling duplicate messages
   - Missing timestamps
   - NULL reaction values
   - Time zone considerations

2. **Performance**
   - Indexing on (sender_id, receiver_id)
   - Partitioning by date
   - Optimizing joins and window functions
   - Handling large datasets

3. **Business Context**
   - Definition of conversation threads
   - Meaningful time windows
   - Reaction significance
   - Activity metrics

4. **Edge Cases**
   - Single message conversations
   - Self-messages (if allowed)
   - Multiple reactions
   - Deleted messages/reactions

## Common Follow-up Questions

1. How would you modify the analysis to:
   - Consider message content types
   - Account for different reaction types
   - Handle group conversations
   - Consider user demographics

2. How would you:
   - Monitor these metrics over time
   - Set up alerts for unusual patterns
   - Scale the analysis for larger datasets
   - Use the insights for product decisions


# App Usage Analysis

## Table Structure
```sql
-- Table for user app sessions
Table: app_usage
user_id | date       | app_id | session_id | duration_seconds
--------------------------------------------------------------------
101     | 2024-01-01 | 1      | 1001      | 300
101     | 2024-01-01 | 2      | 1002      | 600
102     | 2024-01-01 | 1      | 1003      | 450
102     | 2024-01-01 | 3      | 1004      | 900
103     | 2024-01-01 | 2      | 1005      | 750
101     | 2024-01-02 | 1      | 1006      | 400
102     | 2024-01-02 | 2      | 1007      | 500
103     | 2024-01-02 | 3      | 1008      | 800
101     | 2024-01-03 | 1      | 1009      | 350
102     | 2024-01-03 | 1      | 1010      | 600

-- Table for app information
Table: app_info
app_id | app_name     | app_category
--------------------------------------------------------------------
1      | Instagram    | social
2      | Candy Crush  | game
3      | WhatsApp     | social
4      | PUBG         | game
5      | Facebook     | social
6      | Netflix      | entertainment

-- Table for user information
Table: users
user_id | country | age_group | join_date
--------------------------------------------------------------------
101     | US      | 18-24     | 2023-01-01
102     | UK      | 25-34     | 2023-02-15
103     | FR      | 35-44     | 2023-03-20
104     | DE      | 18-24     | 2023-04-10
105     | ES      | 25-34     | 2023-05-05
```

## Interview Questions and Solutions

### 1. Most Used App in Last 30 Days

**Question:** What is the most used app in terms of total time spent over the last 30 days?

**Clarification Points:**
- Define "most used" (time spent vs. number of sessions)
- Handling multiple sessions per user
- Treatment of incomplete or invalid sessions

**Solution:**
```sql
WITH app_time AS (
    SELECT 
        a.app_id,
        ai.app_name,
        SUM(duration_seconds)/3600.0 as total_hours
    FROM app_usage a
    JOIN app_info ai ON a.app_id = ai.app_id
    WHERE date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY a.app_id, ai.app_name
)
SELECT 
    app_name,
    ROUND(total_hours, 2) as hours_spent,
    ROUND(total_hours * 100.0 / SUM(total_hours) OVER (), 2) as percentage_of_total_time
FROM app_time
ORDER BY total_hours DESC
LIMIT 1;
```

### 2. Time Spent Percentage by App Category

**Question:** What is the time spent percentage for each app category in the past 30 days?

**Clarification Points:**
- Handling apps with multiple categories
- Treatment of uncategorized time
- Minimum threshold for significance

**Solution:**
```sql
WITH category_time AS (
    SELECT 
        ai.app_category,
        SUM(duration_seconds)/3600.0 as total_hours
    FROM app_usage a
    JOIN app_info ai ON a.app_id = ai.app_id
    WHERE date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY ai.app_category
)
SELECT 
    app_category,
    ROUND(total_hours, 2) as hours_spent,
    ROUND(total_hours * 100.0 / SUM(total_hours) OVER (), 2) as category_percentage
FROM category_time
ORDER BY total_hours DESC;
```

### 3. Category Engagement Analysis

**Question:** Prove that users of 'social' apps are more engaged than users of 'game' apps.

**Clarification Points:**
- Definition of engagement (daily active users, session frequency, session length)
- Time period for analysis
- Handling users who use both categories

**Solution:**
```sql
WITH user_category_metrics AS (
    SELECT 
        u.user_id,
        ai.app_category,
        COUNT(DISTINCT a.date) as active_days,
        COUNT(*) as total_sessions,
        SUM(duration_seconds)/3600.0 as total_hours,
        COUNT(*) * 1.0 / COUNT(DISTINCT a.date) as sessions_per_active_day,
        SUM(duration_seconds) * 1.0 / COUNT(*) as avg_session_seconds
    FROM app_usage a
    JOIN app_info ai ON a.app_id = ai.app_id
    JOIN users u ON a.user_id = u.user_id
    WHERE 
        ai.app_category IN ('social', 'game')
        AND date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY u.user_id, ai.app_category
)
SELECT 
    app_category,
    COUNT(DISTINCT user_id) as total_users,
    ROUND(AVG(active_days), 2) as avg_active_days,
    ROUND(AVG(sessions_per_active_day), 2) as avg_sessions_per_day,
    ROUND(AVG(avg_session_seconds), 2) as avg_session_duration,
    ROUND(AVG(total_hours), 2) as avg_hours_per_user
FROM user_category_metrics
GROUP BY app_category
ORDER BY avg_active_days DESC;
```

### 4. Retention Analysis by Category

**Question:** Compare 7-day retention between social and game categories.

**Solution:**
```sql
WITH first_use AS (
    SELECT 
        user_id,
        app_category,
        MIN(date) as first_use_date
    FROM app_usage a
    JOIN app_info ai ON a.app_id = ai.app_id
    WHERE app_category IN ('social', 'game')
    GROUP BY user_id, app_category
),
retention AS (
    SELECT 
        f.app_category,
        f.user_id,
        CASE WHEN COUNT(DISTINCT a.date) > 0 THEN 1 ELSE 0 END as retained
    FROM first_use f
    LEFT JOIN app_usage a ON 
        f.user_id = a.user_id
        AND a.date BETWEEN f.first_use_date + INTERVAL '1' DAY 
                      AND f.first_use_date + INTERVAL '7' DAY
    JOIN app_info ai ON a.app_id = ai.app_id AND ai.app_category = f.app_category
    GROUP BY f.app_category, f.user_id
)
SELECT 
    app_category,
    COUNT(*) as total_users,
    SUM(retained) as retained_users,
    ROUND(SUM(retained) * 100.0 / COUNT(*), 2) as retention_rate
FROM retention
GROUP BY app_category;
```

## Key Considerations

1. **Data Quality**
   - Session tracking accuracy
   - Background vs. active usage
   - Multiple device usage
   - Time zone handling

2. **Performance**
   - Indexing on date and user_id
   - Partitioning large tables
   - Optimizing joins
   - Handling large datasets

3. **Business Context**
   - User segments
   - Seasonal patterns
   - Feature releases
   - Competition impact

4. **Edge Cases**
   - App crashes
   - Offline usage
   - Multi-app usage
   - Battery optimization impact

## Common Follow-up Questions

1. How would you:
   - Handle app version changes
   - Account for different platforms
   - Measure feature adoption
   - Track user journey across apps

2. Additional metrics to consider:
   - User acquisition by category
   - Cross-category usage
   - Time of day patterns
   - Session frequency distribution

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_app_usage_date ON app_usage(date);
CREATE INDEX idx_app_usage_user ON app_usage(user_id, date);
CREATE INDEX idx_app_usage_app ON app_usage(app_id, date);
```

2. **Table Partitioning**
```sql
-- Partition by date for better query performance
PARTITION BY RANGE (date);
```

3. **Data Aggregation**
- Consider pre-aggregating daily stats
- Use materialized views for common queries
- Implement appropriate retention policies

---
# Ads Performance and Revenue Analysis

## Table Structure
```sql
-- Table for ad interactions
Table: ad_events
date       | ad_id | user_id | event_type | spend | revenue | platform
-------------------------------------------------------------------------
2024-01-01 | 1     | 101     | 'view'    | 0.50  | 0.00    | 'mobile'
2024-01-01 | 1     | 101     | 'click'   | 0.00  | 2.00    | 'mobile'
2024-01-01 | 2     | 102     | 'view'    | 0.45  | 0.00    | 'desktop'
2024-01-01 | 2     | 102     | 'hide'    | 0.00  | 0.00    | 'desktop'
2024-01-01 | 3     | 103     | 'view'    | 0.55  | 0.00    | 'mobile'
2024-01-01 | 3     | 103     | 'click'   | 0.00  | 3.00    | 'mobile'
2024-01-02 | 1     | 104     | 'view'    | 0.48  | 0.00    | 'desktop'
2024-01-02 | 1     | 104     | 'click'   | 0.00  | 2.50    | 'desktop'
2024-01-02 | 2     | 105     | 'view'    | 0.52  | 0.00    | 'mobile'
2024-01-02 | 3     | 105     | 'hide'    | 0.00  | 0.00    | 'mobile'

-- Table for ad information
Table: ad_details
ad_id | advertiser_id | category    | target_region | bid_amount
-------------------------------------------------------------------------
1     | 201          | 'retail'    | 'US'         | 1.00
2     | 202          | 'tech'      | 'Global'     | 0.80
3     | 203          | 'finance'   | 'EU'         | 1.20
4     | 204          | 'retail'    | 'Asia'       | 0.90
5     | 205          | 'tech'      | 'US'         | 1.10

-- Table for user information
Table: users
user_id | country | age_group | signup_date | last_active
-------------------------------------------------------------------------
101     | 'US'    | '18-24'   | 2023-01-01 | 2024-01-01
102     | 'UK'    | '25-34'   | 2023-02-15 | 2024-01-01
103     | 'FR'    | '35-44'   | 2023-03-20 | 2024-01-01
104     | 'DE'    | '18-24'   | 2023-04-10 | 2024-01-02
105     | 'US'    | '25-34'   | 2023-05-05 | 2024-01-02
```

## Interview Questions and Solutions

### 1. Global Ads Revenue Analysis

**Question:** Calculate the daily global ads revenue and ROI by region for the last 30 days.

**Clarification Points:**
- Revenue definition (clicks, conversions, etc.)
- ROI calculation method
- Regional grouping
- Currency considerations

**Solution:**
```sql
WITH daily_metrics AS (
    SELECT 
        ae.date,
        u.country,
        SUM(ae.spend) as total_spend,
        SUM(ae.revenue) as total_revenue,
        COUNT(DISTINCT CASE WHEN event_type = 'view' THEN ae.user_id END) as total_viewers,
        COUNT(DISTINCT CASE WHEN event_type = 'click' THEN ae.user_id END) as total_clickers
    FROM ad_events ae
    JOIN users u ON ae.user_id = u.user_id
    WHERE ae.date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY ae.date, u.country
)
SELECT 
    date,
    country,
    total_spend,
    total_revenue,
    ROUND((total_revenue - total_spend) * 100.0 / NULLIF(total_spend, 0), 2) as roi_percentage,
    ROUND(total_clickers * 100.0 / NULLIF(total_viewers, 0), 2) as ctr_percentage
FROM daily_metrics
ORDER BY date DESC, roi_percentage DESC;
```

### 2. Best Performing Ads

**Question:** Identify the best performing ads based on CTR, ROI, and engagement metrics.

**Clarification Points:**
- Performance metric definition
- Minimum sample size
- Time period
- Handling regional variations

**Solution:**
```sql
WITH ad_metrics AS (
    SELECT 
        ae.ad_id,
        ad.category,
        COUNT(DISTINCT CASE WHEN event_type = 'view' THEN ae.user_id END) as impressions,
        COUNT(DISTINCT CASE WHEN event_type = 'click' THEN ae.user_id END) as clicks,
        COUNT(DISTINCT CASE WHEN event_type = 'hide' THEN ae.user_id END) as hides,
        SUM(ae.spend) as total_spend,
        SUM(ae.revenue) as total_revenue
    FROM ad_events ae
    JOIN ad_details ad ON ae.ad_id = ad.ad_id
    WHERE ae.date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY ae.ad_id, ad.category
    HAVING COUNT(DISTINCT CASE WHEN event_type = 'view' THEN ae.user_id END) >= 100  -- Minimum sample
)
SELECT 
    ad_id,
    category,
    ROUND(clicks * 100.0 / NULLIF(impressions, 0), 2) as ctr,
    ROUND(hides * 100.0 / NULLIF(impressions, 0), 2) as hide_rate,
    ROUND((total_revenue - total_spend) * 100.0 / NULLIF(total_spend, 0), 2) as roi,
    ROUND(total_revenue / NULLIF(clicks, 0), 2) as revenue_per_click
FROM ad_metrics
ORDER BY (
    (clicks * 1.0 / NULLIF(impressions, 0)) * 0.4 +  -- 40% weight on CTR
    ((total_revenue - total_spend) / NULLIF(total_spend, 0)) * 0.4 +  -- 40% weight on ROI
    (1 - (hides * 1.0 / NULLIF(impressions, 0))) * 0.2  -- 20% weight on engagement
) DESC
LIMIT 10;
```

### 3. Next Ad Recommendation

**Question:** Given a user who clicked on ad_id X, what's the best next ad to show them?

**Clarification Points:**
- Definition of "best next ad"
- User history consideration
- Category affinity
- Time decay factor

**Solution:**
```sql
WITH user_preferences AS (
    -- Calculate user's category preferences
    SELECT 
        ae.user_id,
        ad.category,
        COUNT(DISTINCT CASE WHEN ae.event_type = 'click' THEN ae.ad_id END) as category_clicks,
        COUNT(DISTINCT CASE WHEN ae.event_type = 'hide' THEN ae.ad_id END) as category_hides,
        MAX(ae.date) as last_interaction
    FROM ad_events ae
    JOIN ad_details ad ON ae.ad_id = ad.ad_id
    GROUP BY ae.user_id, ad.category
),
ad_performance AS (
    -- Calculate overall ad performance
    SELECT 
        ad_id,
        category,
        COUNT(DISTINCT CASE WHEN event_type = 'click' THEN user_id END) * 1.0 /
        NULLIF(COUNT(DISTINCT CASE WHEN event_type = 'view' THEN user_id END), 0) as ctr
    FROM ad_events ae
    JOIN ad_details ad ON ae.ad_id = ad.ad_id
    GROUP BY ad_id, category
    HAVING COUNT(DISTINCT CASE WHEN event_type = 'view' THEN user_id END) >= 100
)
SELECT 
    ap.ad_id,
    ap.category,
    ap.ctr as overall_ctr,
    up.category_clicks,
    up.category_hides,
    ROUND(
        (ap.ctr * 0.4 +  -- 40% weight on overall ad performance
         (up.category_clicks * 1.0 / NULLIF(up.category_clicks + up.category_hides, 0)) * 0.4 +  -- 40% weight on user preference
         CASE WHEN up.last_interaction >= CURRENT_DATE - INTERVAL '7' DAY THEN 0.2 ELSE 0.1 END  -- recency bonus
        ) * 100,
        2
    ) as recommendation_score
FROM ad_performance ap
JOIN user_preferences up ON ap.category = up.category
WHERE up.user_id = :user_id  -- Parameter for specific user
  AND ap.ad_id NOT IN (  -- Exclude already shown ads
    SELECT ad_id 
    FROM ad_events 
    WHERE user_id = :user_id
  )
ORDER BY recommendation_score DESC
LIMIT 5;
```

## Key Considerations

1. **Data Quality**
   - Tracking accuracy
   - Bot traffic
   - Invalid clicks
   - Attribution windows

2. **Performance**
   - High write volume
   - Real-time requirements
   - Data aggregation
   - Indexing strategy

3. **Business Context**
   - Advertiser goals
   - User experience
   - Platform policies
   - Competition

4. **Edge Cases**
   - New users/ads
   - Regional differences
   - Seasonal patterns
   - Campaign launches

## Follow-up Questions

1. How would you:
   - Handle fraudulent activity
   - Optimize for different objectives
   - Scale recommendations
   - Measure long-term impact

2. Additional considerations:
   - Budget pacing
   - Audience targeting
   - Creative optimization
   - A/B testing strategy

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_ad_events_date ON ad_events(date);
CREATE INDEX idx_ad_events_user ON ad_events(user_id, date);
CREATE INDEX idx_ad_events_ad ON ad_events(ad_id, event_type);
```

2. **Partitioning**
```sql
-- Partition by date for better query performance
PARTITION BY RANGE (date);
```

3. **Materialized Views**
```sql
-- Create daily aggregates
CREATE MATERIALIZED VIEW daily_ad_metrics AS
SELECT 
    date,
    ad_id,
    COUNT(DISTINCT CASE WHEN event_type = 'view' THEN user_id END) as daily_views,
    COUNT(DISTINCT CASE WHEN event_type = 'click' THEN user_id END) as daily_clicks
FROM ad_events
GROUP BY date, ad_id;
```  

---
# Search Quality Measurement

## Table Structure
```sql
-- Table for search queries
Table: searches
search_id | user_id | query_text           | timestamp           | device_type
--------------------------------------------------------------------------------
1001     | 101     | 'machine learning'    | 2024-01-01 10:00:00| 'mobile'
1002     | 102     | 'data science jobs'   | 2024-01-01 10:05:00| 'desktop'
1003     | 103     | 'python tutorial'     | 2024-01-01 10:10:00| 'mobile'
1004     | 101     | 'sql interview prep'  | 2024-01-01 11:00:00| 'mobile'
1005     | 104     | 'machine learning'    | 2024-01-01 11:30:00| 'desktop'

-- Table for search results and interactions
Table: search_results
search_id | result_id | position | relevance_score | clicked | dwell_time_seconds
--------------------------------------------------------------------------------
1001     | 5001      | 1        | 0.95           | TRUE    | 120
1001     | 5002      | 2        | 0.85           | FALSE   | 0
1001     | 5003      | 3        | 0.75           | FALSE   | 0
1002     | 5004      | 1        | 0.90           | TRUE    | 180
1002     | 5005      | 2        | 0.88           | TRUE    | 90
1003     | 5006      | 1        | 0.92           | FALSE   | 0
1003     | 5007      | 2        | 0.78           | TRUE    | 150
1004     | 5008      | 1        | 0.94           | TRUE    | 200
1005     | 5009      | 1        | 0.95           | TRUE    | 160

-- Table for result content information
Table: result_content
result_id | content_type | title                    | description
--------------------------------------------------------------------------------
5001     | 'article'    | 'Intro to ML'            | 'Beginner ML guide'
5002     | 'video'      | 'ML Tutorial'            | 'Video series on ML'
5003     | 'course'     | 'ML Certification'       | 'Professional course'
5004     | 'job'        | 'Data Scientist Opening' | 'Senior DS position'
5005     | 'job'        | 'ML Engineer Role'       | 'ML engineering job'
```

## Interview Questions and Solutions

### 1. Basic Search Quality Score

**Question:** Define and calculate a search quality metric that considers both position and relevance of results.

**Clarification Points:**
- Position importance (decay factor)
- Relevance score weighting
- Click behavior influence
- Minimum quality threshold

**Solution:**
```sql
WITH position_weighted_scores AS (
    SELECT 
        s.search_id,
        s.query_text,
        -- Calculate position-weighted score using log decay
        SUM(
            sr.relevance_score * (1.0 / LOG(2 + sr.position)) * 
            CASE WHEN sr.clicked THEN 1.5 ELSE 1.0 END
        ) as weighted_score,
        -- Count results for normalization
        COUNT(*) as num_results,
        -- Calculate click-through rate
        AVG(CASE WHEN sr.clicked THEN 1.0 ELSE 0.0 END) as ctr
    FROM searches s
    JOIN search_results sr ON s.search_id = sr.search_id
    WHERE sr.position <= 10  -- Consider top 10 results
    GROUP BY s.search_id, s.query_text
)
SELECT 
    query_text,
    ROUND(AVG(weighted_score), 4) as avg_quality_score,
    ROUND(AVG(ctr), 4) as avg_ctr,
    COUNT(DISTINCT search_id) as query_frequency
FROM position_weighted_scores
GROUP BY query_text
HAVING COUNT(DISTINCT search_id) >= 5  -- Minimum sample size
ORDER BY avg_quality_score DESC;
```

### 2. Advanced Quality Metrics

**Question:** Calculate comprehensive search quality metrics including position-normalized DCG (Discounted Cumulative Gain) and user engagement factors.

**Solution:**
```sql
WITH result_metrics AS (
    SELECT 
        s.search_id,
        s.query_text,
        sr.position,
        sr.relevance_score,
        sr.clicked,
        sr.dwell_time_seconds,
        -- Calculate DCG components
        relevance_score / LOG(2 + position) as dcg_score,
        -- Calculate engagement score
        CASE 
            WHEN clicked AND dwell_time_seconds >= 30 THEN 1.0
            WHEN clicked AND dwell_time_seconds >= 10 THEN 0.5
            ELSE 0.0 
        END as engagement_score
    FROM searches s
    JOIN search_results sr ON s.search_id = sr.search_id
    WHERE sr.position <= 10
),
query_metrics AS (
    SELECT 
        query_text,
        COUNT(DISTINCT search_id) as query_count,
        -- Calculate average DCG
        AVG(dcg_score) as avg_dcg,
        -- Calculate engagement metrics
        AVG(engagement_score) as avg_engagement,
        -- Calculate position-weighted relevance
        AVG(relevance_score * (1.0 / position)) as position_weighted_relevance
    FROM result_metrics
    GROUP BY query_text
    HAVING COUNT(DISTINCT search_id) >= 5
)
SELECT 
    query_text,
    query_count,
    ROUND(avg_dcg, 4) as normalized_dcg,
    ROUND(avg_engagement, 4) as engagement_score,
    ROUND(position_weighted_relevance, 4) as weighted_relevance,
    ROUND(
        (avg_dcg * 0.4 + 
         avg_engagement * 0.4 + 
         position_weighted_relevance * 0.2
        ), 4
    ) as overall_quality_score
FROM query_metrics
ORDER BY overall_quality_score DESC;
```

### 3. Query-Specific Performance Analysis

**Question:** Analyze search quality variations across different query types and user segments.

**Solution:**
```sql
WITH query_segments AS (
    SELECT 
        s.search_id,
        s.query_text,
        s.device_type,
        -- Categorize queries by length
        CASE 
            WHEN LENGTH(s.query_text) <= 10 THEN 'short'
            WHEN LENGTH(s.query_text) <= 30 THEN 'medium'
            ELSE 'long'
        END as query_length_category,
        -- Calculate quality metrics
        AVG(sr.relevance_score) as avg_relevance,
        SUM(CASE WHEN sr.clicked THEN 1 ELSE 0 END) * 1.0 / 
            COUNT(*) as click_rate,
        AVG(CASE 
            WHEN sr.clicked THEN sr.dwell_time_seconds 
            ELSE 0 
        END) as avg_dwell_time
    FROM searches s
    JOIN search_results sr ON s.search_id = sr.search_id
    GROUP BY 
        s.search_id,
        s.query_text,
        s.device_type
)
SELECT 
    device_type,
    query_length_category,
    COUNT(*) as search_count,
    ROUND(AVG(avg_relevance), 4) as avg_relevance_score,
    ROUND(AVG(click_rate), 4) as avg_click_rate,
    ROUND(AVG(avg_dwell_time), 2) as avg_dwell_time_seconds
FROM query_segments
GROUP BY 
    device_type,
    query_length_category
ORDER BY 
    device_type,
    query_length_category;
```

## Key Considerations

1. **Metric Components**
   - Position decay factor
   - Relevance scores
   - Click behavior
   - Dwell time
   - Query characteristics

2. **Quality Factors**
   - Result diversity
   - User satisfaction
   - Query intent matching
   - Result freshness

3. **Edge Cases**
   - Zero-result queries
   - Misspellings
   - Ambiguous queries
   - Seasonal queries

4. **Business Impact**
   - User retention
   - Search abandonment
   - Query reformulation
   - Result diversity

## Follow-up Questions

1. How would you:
   - Handle different languages?
   - Account for personalization?
   - Measure long-tail queries?
   - Compare across markets?

2. Consider:
   - A/B testing methodology
   - Seasonality effects
   - Market differences
   - User segments

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_search_results_composite 
ON search_results(search_id, position, relevance_score);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW daily_search_metrics AS
SELECT 
    DATE_TRUNC('day', timestamp) as search_date,
    COUNT(DISTINCT search_id) as total_searches,
    AVG(relevance_score) as avg_relevance
FROM searches s
JOIN search_results sr ON s.search_id = sr.search_id
GROUP BY DATE_TRUNC('day', timestamp);
```
---
# User Activity Analysis 
## Table Structure
```sql
-- Table for user information
Table: users
user_id | signup_date | country | platform | acquisition_source
--------------------------------------------------------------------------------
101     | 2023-12-01 | 'US'    | 'ios'    | 'organic'
102     | 2023-12-15 | 'UK'    | 'android'| 'paid'
103     | 2024-01-01 | 'FR'    | 'web'    | 'referral'
104     | 2024-01-01 | 'DE'    | 'ios'    | 'organic'
105     | 2024-01-15 | 'US'    | 'android'| 'paid'

-- Table for user activities
Table: user_actions
user_id | action_date         | action_type | content_id | session_duration_sec
--------------------------------------------------------------------------------
101     | 2024-01-01 10:00:00| 'post'     | 1001      | 120
101     | 2024-01-01 10:30:00| 'like'     | 1002      | 60
102     | 2024-01-01 11:00:00| 'comment'  | 1001      | 180
103     | 2024-01-01 12:00:00| 'view'     | 1003      | 300
103     | 2024-01-01 12:30:00| 'share'    | 1003      | 90
104     | 2024-01-01 13:00:00| 'post'     | 1004      | 150
105     | 2024-01-15 14:00:00| 'view'     | 1005      | 200

-- Table for engagement metrics
Table: daily_engagement
user_id | date       | sessions | total_time_sec | content_created | interactions
--------------------------------------------------------------------------------
101     | 2024-01-01| 3        | 600           | 1              | 5
102     | 2024-01-01| 2        | 450           | 0              | 3
103     | 2024-01-01| 4        | 800           | 0              | 2
104     | 2024-01-01| 1        | 300           | 1              | 1
105     | 2024-01-15| 2        | 400           | 0              | 2
```

## Interview Questions and Solutions

### 1. Define and Compare New vs. Existing Users

**Question:** Are existing users more active than new users? Define what constitutes a "new user" and "active user", then analyze the difference.

**Clarification Points:**
- New user definition (e.g., < 30 days since signup)
- Activity metrics to consider
- Minimum activity threshold
- Time window for analysis

**Solution:**
```sql
WITH user_categories AS (
    SELECT 
        u.user_id,
        u.signup_date,
        -- Define user type (new vs existing)
        CASE 
            WHEN DATE_DIFF('day', u.signup_date, CURRENT_DATE) <= 30 THEN 'new'
            ELSE 'existing'
        END as user_type,
        -- Calculate activity metrics
        COUNT(DISTINCT ua.action_date) as active_days,
        COUNT(ua.action_type) as total_actions,
        SUM(CASE WHEN ua.action_type = 'post' THEN 1 ELSE 0 END) as posts_created,
        SUM(session_duration_sec) / 3600.0 as total_hours,
        -- Calculate engagement ratios
        COUNT(ua.action_type) * 1.0 / 
            COUNT(DISTINCT ua.action_date) as actions_per_day,
        SUM(session_duration_sec) * 1.0 / 
            COUNT(DISTINCT ua.action_date) as avg_daily_duration
    FROM users u
    LEFT JOIN user_actions ua 
        ON u.user_id = ua.user_id
        AND ua.action_date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY 
        u.user_id,
        u.signup_date
)
SELECT 
    user_type,
    COUNT(*) as user_count,
    ROUND(AVG(active_days), 2) as avg_active_days,
    ROUND(AVG(total_actions), 2) as avg_total_actions,
    ROUND(AVG(posts_created), 2) as avg_posts_created,
    ROUND(AVG(total_hours), 2) as avg_hours_spent,
    ROUND(AVG(actions_per_day), 2) as avg_actions_per_day,
    ROUND(AVG(avg_daily_duration), 2) as avg_daily_duration_sec
FROM user_categories
GROUP BY user_type
ORDER BY user_type;
```

### 2. Detailed Activity Analysis

**Question:** Analyze user activity patterns and engagement levels by user type and acquisition source.

**Solution:**
```sql
WITH user_metrics AS (
    SELECT 
        u.user_id,
        u.acquisition_source,
        DATE_DIFF('day', u.signup_date, CURRENT_DATE) as account_age_days,
        -- Calculate rolling 7-day activity
        COUNT(DISTINCT CASE 
            WHEN ua.action_date >= CURRENT_DATE - INTERVAL '7' DAY 
            THEN ua.action_date 
        END) as active_days_7d,
        -- Calculate engagement metrics
        COUNT(DISTINCT CASE 
            WHEN ua.action_type IN ('post', 'comment') 
            THEN ua.action_date 
        END) as content_creation_days,
        SUM(ua.session_duration_sec) as total_time_spent,
        -- Calculate retention indicators
        MAX(ua.action_date) as last_activity_date
    FROM users u
    LEFT JOIN user_actions ua ON u.user_id = ua.user_id
    GROUP BY u.user_id, u.acquisition_source, u.signup_date
),
user_segments AS (
    SELECT 
        user_id,
        acquisition_source,
        account_age_days,
        -- Define user segments
        CASE 
            WHEN account_age_days <= 7 THEN 'week1'
            WHEN account_age_days <= 30 THEN 'month1'
            WHEN account_age_days <= 90 THEN 'month2_3'
            ELSE 'older'
        END as age_segment,
        -- Calculate engagement levels
        CASE 
            WHEN active_days_7d >= 5 THEN 'highly_active'
            WHEN active_days_7d >= 2 THEN 'active'
            WHEN active_days_7d >= 1 THEN 'low_active'
            ELSE 'inactive'
        END as activity_level,
        content_creation_days,
        total_time_spent
    FROM user_metrics
)
SELECT 
    age_segment,
    acquisition_source,
    activity_level,
    COUNT(*) as user_count,
    ROUND(AVG(content_creation_days), 2) as avg_content_days,
    ROUND(AVG(total_time_spent)/3600.0, 2) as avg_hours_spent,
    ROUND(COUNT(*) * 100.0 / 
        SUM(COUNT(*)) OVER (PARTITION BY age_segment), 2
    ) as segment_percentage
FROM user_segments
GROUP BY age_segment, acquisition_source, activity_level
ORDER BY 
    age_segment,
    acquisition_source,
    COUNT(*) DESC;
```

### 3. Retention Analysis by User Type

**Question:** Compare retention rates between new and existing users across different time periods.

**Solution:**
```sql
WITH cohort_activity AS (
    SELECT 
        u.user_id,
        u.signup_date,
        DATE_DIFF('day', u.signup_date, CURRENT_DATE) as account_age_days,
        DATE_DIFF('day', u.signup_date, ua.action_date) as day_number,
        COUNT(DISTINCT ua.action_date) as active_days,
        SUM(ua.session_duration_sec) as total_time
    FROM users u
    LEFT JOIN user_actions ua ON u.user_id = ua.user_id
    GROUP BY 
        u.user_id,
        u.signup_date,
        DATE_DIFF('day', u.signup_date, ua.action_date)
),
retention_metrics AS (
    SELECT 
        CASE 
            WHEN account_age_days <= 30 THEN 'new'
            ELSE 'existing'
        END as user_type,
        day_number,
        COUNT(DISTINCT user_id) as users_active,
        SUM(total_time) as total_time_spent
    FROM cohort_activity
    WHERE day_number <= 30  -- Look at first 30 days
    GROUP BY 
        CASE 
            WHEN account_age_days <= 30 THEN 'new'
            ELSE 'existing'
        END,
        day_number
)
SELECT 
    user_type,
    day_number,
    users_active,
    ROUND(
        users_active * 100.0 / 
        FIRST_VALUE(users_active) OVER (
            PARTITION BY user_type 
            ORDER BY day_number
        ),
        2
    ) as retention_rate,
    ROUND(total_time_spent / NULLIF(users_active, 0) / 3600.0, 2) as avg_hours_per_user
FROM retention_metrics
ORDER BY 
    user_type,
    day_number;
```

## Key Considerations

1. **User Definitions**
   - Clear new vs existing criteria
   - Activity thresholds
   - Engagement levels
   - Retention periods

2. **Activity Metrics**
   - Session frequency
   - Session duration
   - Content creation
   - Social interactions

3. **Edge Cases**
   - Inactive users
   - Seasonal patterns
   - Platform differences
   - Regional variations

4. **Business Impact**
   - Growth metrics
   - Retention strategies
   - Feature adoption
   - User satisfaction

## Follow-up Questions

1. How would you:
   - Account for different time zones?
   - Handle reactivated users?
   - Measure quality of engagement?
   - Compare across platforms?

2. Consider:
   - Seasonal effects
   - Market differences
   - Feature releases
   - Marketing campaigns

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_user_actions_composite 
ON user_actions(user_id, action_date, action_type);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW daily_user_metrics AS
SELECT 
    user_id,
    DATE_TRUNC('day', action_date) as activity_date,
    COUNT(*) as daily_actions,
    SUM(session_duration_sec) as daily_duration
FROM user_actions
GROUP BY user_id, DATE_TRUNC('day', action_date);
```

3. **Query Optimization**
- Use appropriate partitioning
- Pre-calculate common metrics
- Implement efficient joins
- Consider data sampling for large-scale analysis

3. **Query Optimization**
- Use appropriate partitioning
- Pre-calculate common metrics
- Implement efficient joins
- Consider data sampling for large-scale analysis
  
---
# Comment Distribution

## Table Structure
```sql
-- Table for posts
Table: posts
post_id | user_id | post_date           | content_type | engagement_score
--------------------------------------------------------------------------------
1001    | 101     | 2024-01-01 10:00:00| 'text'      | 85
1002    | 102     | 2024-01-01 11:00:00| 'photo'     | 92
1003    | 101     | 2024-01-01 12:00:00| 'video'     | 78
1004    | 103     | 2024-01-01 13:00:00| 'text'      | 65
1005    | 104     | 2024-01-01 14:00:00| 'photo'     | 88

-- Table for comments
Table: comments
comment_id | post_id | user_id | comment_date         | comment_text    | parent_comment_id
------------------------------------------------------------------------------------------
5001      | 1001    | 201     | 2024-01-01 10:05:00 | 'Great post!'   | NULL
5002      | 1001    | 202     | 2024-01-01 10:10:00 | 'Agreed!'       | 5001
5003      | 1001    | 203     | 2024-01-01 10:15:00 | 'Thanks!'       | NULL
5004      | 1002    | 204     | 2024-01-01 11:05:00 | 'Nice photo!'   | NULL
5005      | 1002    | 205     | 2024-01-01 11:10:00 | 'Beautiful!'    | NULL
5006      | 1003    | 206     | 2024-01-01 12:05:00 | 'Cool video'    | NULL
5007      | 1005    | 207     | 2024-01-01 14:05:00 | 'Amazing shot!' | NULL
5008      | 1005    | 208     | 2024-01-01 14:10:00 | 'Where is this?'| 5007
5009      | 1005    | 209     | 2024-01-01 14:15:00 | 'Love it!'      | NULL

-- Table for user information
Table: users
user_id | join_date  | country | account_type
--------------------------------------------------------------------------------
101     | 2023-01-01| 'US'    | 'creator'
102     | 2023-02-01| 'UK'    | 'regular'
103     | 2023-03-01| 'FR'    | 'regular'
104     | 2023-04-01| 'DE'    | 'creator'
201     | 2023-05-01| 'US'    | 'regular'
```

## Interview Questions and Solutions

### 1. Basic Comment Distribution Metrics

**Question:** Calculate the distribution of comments per post, including P5, P25, P50 (median), P75, P95, and mean.

**Solution:**
```sql
WITH comment_counts AS (
    SELECT 
        p.post_id,
        p.content_type,
        COUNT(c.comment_id) as comment_count
    FROM posts p
    LEFT JOIN comments c ON p.post_id = c.post_id
    GROUP BY p.post_id, p.content_type
)
SELECT 
    -- Basic statistics
    COUNT(*) as total_posts,
    ROUND(AVG(comment_count), 2) as mean_comments,
    -- Percentiles
    PERCENTILE_CONT(0.05) WITHIN GROUP (ORDER BY comment_count) as p5,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY comment_count) as p25,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY comment_count) as median,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY comment_count) as p75,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY comment_count) as p95,
    -- Additional statistics
    MIN(comment_count) as min_comments,
    MAX(comment_count) as max_comments,
    ROUND(STDDEV(comment_count), 2) as std_dev
FROM comment_counts;
```

### 2. Comment Distribution by Content Type

**Question:** Analyze how comment distribution varies across different types of content.

**Solution:**
```sql
WITH comment_counts AS (
    SELECT 
        p.post_id,
        p.content_type,
        COUNT(c.comment_id) as comment_count
    FROM posts p
    LEFT JOIN comments c ON p.post_id = c.post_id
    GROUP BY p.post_id, p.content_type
)
SELECT 
    content_type,
    COUNT(*) as post_count,
    ROUND(AVG(comment_count), 2) as mean_comments,
    -- Percentiles by content type
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY comment_count) as p25,
    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY comment_count) as median,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY comment_count) as p75,
    -- Distribution metrics
    ROUND(STDDEV(comment_count), 2) as std_dev,
    ROUND(
        (PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY comment_count) -
         PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY comment_count)) * 1.0 /
        NULLIF(PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY comment_count), 0),
        2
    ) as coefficient_of_variation
FROM comment_counts
GROUP BY content_type
ORDER BY mean_comments DESC;
```

### 3. Time-Based Comment Distribution

**Question:** Analyze how comment distributions vary over time and by post age.

**Solution:**
```sql
WITH post_age_comments AS (
    SELECT 
        p.post_id,
        p.post_date,
        c.comment_date,
        EXTRACT(EPOCH FROM (c.comment_date - p.post_date))/3600 as hours_since_post
    FROM posts p
    LEFT JOIN comments c ON p.post_id = c.post_id
),
hourly_distributions AS (
    SELECT 
        FLOOR(hours_since_post) as hour_bucket,
        COUNT(*) as comment_count
    FROM post_age_comments
    WHERE hours_since_post <= 24  -- First 24 hours
    GROUP BY FLOOR(hours_since_post)
),
cumulative_stats AS (
    SELECT 
        hour_bucket,
        comment_count,
        SUM(comment_count) OVER (ORDER BY hour_bucket) as cumulative_comments,
        SUM(comment_count) OVER () as total_comments
    FROM hourly_distributions
)
SELECT 
    hour_bucket,
    comment_count as hourly_comments,
    cumulative_comments,
    ROUND(
        cumulative_comments * 100.0 / NULLIF(total_comments, 0),
        2
    ) as cumulative_percentage
FROM cumulative_stats
ORDER BY hour_bucket;
```

### 4. Comment Depth Analysis

**Question:** Analyze the distribution of comment threads and reply depths.

**Solution:**
```sql
WITH RECURSIVE comment_thread AS (
    -- Base case: top-level comments
    SELECT 
        comment_id,
        post_id,
        parent_comment_id,
        1 as thread_depth
    FROM comments
    WHERE parent_comment_id IS NULL
    
    UNION ALL
    
    -- Recursive case: replies
    SELECT 
        c.comment_id,
        c.post_id,
        c.parent_comment_id,
        ct.thread_depth + 1
    FROM comments c
    JOIN comment_thread ct ON c.parent_comment_id = ct.comment_id
),
thread_metrics AS (
    SELECT 
        post_id,
        MAX(thread_depth) as max_depth,
        COUNT(*) as thread_size
    FROM comment_thread
    GROUP BY post_id
)
SELECT 
    max_depth,
    COUNT(*) as thread_count,
    ROUND(AVG(thread_size), 2) as avg_thread_size,
    ROUND(
        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (),
        2
    ) as percentage_of_threads
FROM thread_metrics
GROUP BY max_depth
ORDER BY max_depth;
```

## Key Considerations

1. **Statistical Measures**
   - Basic metrics (mean, median)
   - Percentiles (P5, P25, P50, P75, P95)
   - Variance and standard deviation
   - Distribution shape

2. **Analysis Dimensions**
   - Content type
   - Time of day
   - User segments
   - Post age

3. **Edge Cases**
   - Posts with no comments
   - Deleted comments
   - Spam comments
   - Nested replies

4. **Business Impact**
   - Engagement patterns
   - Content quality
   - User behavior
   - Platform health

## Follow-up Questions

1. How would you:
   - Handle spam comments?
   - Account for deleted content?
   - Measure quality vs. quantity?
   - Compare across regions?

2. Consider:
   - Seasonal patterns
   - Viral content
   - User segments
   - Content categories

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_comments_post ON comments(post_id);
CREATE INDEX idx_comments_user ON comments(user_id);
CREATE INDEX idx_comments_date ON comments(comment_date);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW hourly_comment_stats AS
SELECT 
    post_id,
    DATE_TRUNC('hour', comment_date) as hour,
    COUNT(*) as comment_count
FROM comments
GROUP BY post_id, DATE_TRUNC('hour', comment_date);
```

3. **Partitioning Strategy**
```sql
-- Partition comments table by date for better performance
PARTITION BY RANGE (comment_date);
```

---
# Spam and Fraud Detection Analysis

## Table Structure
```sql
-- Table for user accounts
Table: accounts
account_id | create_date | status     | risk_score | country | device_id
--------------------------------------------------------------------------------
1001      | 2024-01-01  | 'active'   | 0.15      | 'US'    | 'd101'
1002      | 2024-01-01  | 'banned'   | 0.85      | 'UK'    | 'd102'
1003      | 2024-01-01  | 'active'   | 0.25      | 'FR'    | 'd103'
1004      | 2024-01-02  | 'fraud'    | 0.92      | 'US'    | 'd102'
1005      | 2024-01-02  | 'active'   | 0.18      | 'DE'    | 'd104'

-- Table for account activity
Table: activities
activity_id | account_id | activity_date       | activity_type | target_id | amount
--------------------------------------------------------------------------------
5001       | 1001       | 2024-01-01 10:00:00| 'post'       | NULL      | NULL
5002       | 1002       | 2024-01-01 10:05:00| 'ad_click'   | 'ad123'   | 10.50
5003       | 1002       | 2024-01-01 10:10:00| 'ad_click'   | 'ad124'   | 15.75
5004       | 1003       | 2024-01-01 11:00:00| 'message'    | 2001      | NULL
5005       | 1004       | 2024-01-02 12:00:00| 'ad_click'   | 'ad125'   | 20.25

-- Table for reports
Table: reports
report_id | reporter_id | reported_id | report_date        | report_type  | action_taken
--------------------------------------------------------------------------------
7001     | 2001        | 1002        | 2024-01-01 15:00:00| 'spam'      | 'banned'
7002     | 2002        | 1002        | 2024-01-01 15:30:00| 'fraud'     | 'banned'
7003     | 2003        | 1004        | 2024-01-02 16:00:00| 'spam'      | 'warning'
7004     | 2004        | 1004        | 2024-01-02 16:30:00| 'fraud'     | 'banned'
7005     | 2005        | 1003        | 2024-01-02 17:00:00| 'spam'      | 'none'

-- Table for ad interactions
Table: ad_interactions
ad_id    | account_id | interaction_date    | interaction_type | spend  | revenue
--------------------------------------------------------------------------------
'ad123'  | 1001       | 2024-01-01 10:00:00| 'impression'    | 0.50   | 0.00
'ad123'  | 1002       | 2024-01-01 10:05:00| 'click'        | 0.00   | 10.50
'ad124'  | 1002       | 2024-01-01 10:10:00| 'click'        | 0.00   | 15.75
'ad125'  | 1004       | 2024-01-02 12:00:00| 'click'        | 0.00   | 20.25
'ad126'  | 1005       | 2024-01-02 13:00:00| 'impression'   | 0.45   | 0.00
```

## Interview Questions and Solutions

### 1. Identify Potentially Fraudulent Accounts

**Question:** Calculate the percentage of accounts that were banned for fraud in the last month and identify patterns among fraudulent accounts.

**Solution:**
```sql
WITH account_metrics AS (
    SELECT 
        a.account_id,
        a.create_date,
        a.status,
        a.device_id,
        -- Activity patterns
        COUNT(DISTINCT act.activity_date) as active_days,
        COUNT(act.activity_id) as total_activities,
        SUM(CASE WHEN act.activity_type = 'ad_click' THEN 1 ELSE 0 END) as ad_clicks,
        -- Report patterns
        COUNT(DISTINCT r.report_id) as times_reported,
        COUNT(DISTINCT r.reporter_id) as unique_reporters,
        -- Device usage
        COUNT(DISTINCT a2.account_id) as accounts_on_device
    FROM accounts a
    LEFT JOIN activities act ON a.account_id = act.account_id
    LEFT JOIN reports r ON a.account_id = r.reported_id
    LEFT JOIN accounts a2 ON a.device_id = a2.device_id
    WHERE a.create_date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY a.account_id, a.create_date, a.status, a.device_id
)
SELECT 
    -- Fraud rate
    ROUND(
        COUNT(CASE WHEN status IN ('banned', 'fraud') THEN 1 END) * 100.0 / 
        COUNT(*),
        2
    ) as fraud_rate,
    -- Activity patterns
    ROUND(AVG(CASE WHEN status IN ('banned', 'fraud') 
        THEN total_activities END), 2) as avg_fraud_activities,
    ROUND(AVG(CASE WHEN status = 'active' 
        THEN total_activities END), 2) as avg_normal_activities,
    -- Device patterns
    ROUND(AVG(CASE WHEN status IN ('banned', 'fraud') 
        THEN accounts_on_device END), 2) as avg_fraud_accounts_per_device,
    -- Report patterns
    ROUND(AVG(CASE WHEN status IN ('banned', 'fraud') 
        THEN times_reported END), 2) as avg_fraud_reports
FROM account_metrics;
```

### 2. Detect Ad Click Fraud

**Question:** Identify suspicious ad clicking patterns and potential click fraud.

**Solution:**
```sql
WITH click_patterns AS (
    SELECT 
        ai.account_id,
        a.status,
        COUNT(*) as total_clicks,
        COUNT(DISTINCT ad_id) as unique_ads_clicked,
        MIN(interaction_date) as first_click,
        MAX(interaction_date) as last_click,
        SUM(revenue) as total_revenue,
        -- Calculate time-based metrics
        AVG(EXTRACT(EPOCH FROM (
            interaction_date - LAG(interaction_date) 
            OVER (PARTITION BY account_id ORDER BY interaction_date)
        ))) as avg_seconds_between_clicks
    FROM ad_interactions ai
    JOIN accounts a ON ai.account_id = a.account_id
    WHERE 
        interaction_type = 'click'
        AND interaction_date >= CURRENT_DATE - INTERVAL '7' DAY
    GROUP BY ai.account_id, a.status
),
suspicious_patterns AS (
    SELECT 
        account_id,
        CASE
            WHEN total_clicks >= 100 THEN 1
            WHEN avg_seconds_between_clicks <= 10 THEN 1
            WHEN total_clicks/NULLIF(unique_ads_clicked, 0) >= 10 THEN 1
            ELSE 0
        END as is_suspicious
    FROM click_patterns
)
SELECT 
    ROUND(AVG(is_suspicious) * 100, 2) as suspicious_account_percentage,
    COUNT(CASE WHEN is_suspicious = 1 THEN 1 END) as suspicious_accounts,
    COUNT(*) as total_accounts
FROM suspicious_patterns;
```

### 3. Multiple Account Detection

**Question:** Identify clusters of related accounts that might belong to the same bad actor.

**Solution:**
```sql
WITH account_relationships AS (
    SELECT 
        a1.account_id as account1,
        a2.account_id as account2,
        -- Calculate similarity score based on various factors
        CASE 
            WHEN a1.device_id = a2.device_id THEN 1
            ELSE 0
        END as device_match,
        CASE 
            WHEN ABS(EXTRACT(EPOCH FROM (a1.create_date - a2.create_date))) <= 3600 THEN 1
            ELSE 0
        END as creation_time_match,
        CASE 
            WHEN a1.country = a2.country THEN 1
            ELSE 0
        END as country_match
    FROM accounts a1
    JOIN accounts a2 
    ON a1.account_id < a2.account_id  -- Avoid self-joins and duplicates
    WHERE 
        a1.create_date >= CURRENT_DATE - INTERVAL '30' DAY
        AND a2.create_date >= CURRENT_DATE - INTERVAL '30' DAY
),
account_clusters AS (
    SELECT 
        account1,
        account2,
        device_match + creation_time_match + country_match as similarity_score
    FROM account_relationships
    WHERE device_match + creation_time_match + country_match >= 2  -- Minimum similarity threshold
)
SELECT 
    a.account_id,
    COUNT(DISTINCT ac.account2) as related_accounts,
    STRING_AGG(CAST(ac.account2 AS VARCHAR), ', ') as related_account_ids,
    MAX(a.risk_score) as max_risk_score
FROM accounts a
JOIN account_clusters ac ON a.account_id = ac.account1
GROUP BY a.account_id
HAVING COUNT(DISTINCT ac.account2) >= 2
ORDER BY related_accounts DESC;
```

### 4. Report Analysis for Dating App

**Question:** Analyze patterns in reported profiles to identify systematic abuse.

**Solution:**
```sql
WITH report_patterns AS (
    SELECT 
        reported_id,
        COUNT(DISTINCT report_id) as total_reports,
        COUNT(DISTINCT reporter_id) as unique_reporters,
        COUNT(DISTINCT report_type) as unique_report_types,
        STRING_AGG(DISTINCT report_type, ', ') as report_types,
        COUNT(CASE WHEN action_taken = 'banned' THEN 1 END) as bans,
        MIN(report_date) as first_report,
        MAX(report_date) as last_report
    FROM reports
    WHERE report_date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY reported_id
),
account_activity AS (
    SELECT 
        account_id,
        COUNT(*) as total_activities,
        COUNT(CASE WHEN activity_type = 'message' THEN 1 END) as message_count,
        COUNT(DISTINCT DATE(activity_date)) as active_days
    FROM activities
    WHERE activity_date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY account_id
)
SELECT 
    rp.reported_id,
    rp.total_reports,
    rp.unique_reporters,
    rp.report_types,
    rp.bans,
    aa.total_activities,
    aa.message_count,
    ROUND(
        rp.unique_reporters * 100.0 / NULLIF(aa.message_count, 0),
        2
    ) as report_rate,
    CASE 
        WHEN rp.unique_reporters >= 5 AND rp.bans >= 2 THEN 'High Risk'
        WHEN rp.unique_reporters >= 3 OR rp.bans >= 1 THEN 'Medium Risk'
        ELSE 'Low Risk'
    END as risk_category
FROM report_patterns rp
JOIN account_activity aa ON rp.reported_id = aa.account_id
WHERE rp.unique_reporters >= 2  -- Minimum threshold for analysis
ORDER BY rp.total_reports DESC;
```

## Key Considerations

1. **Pattern Detection**
   - Activity frequency
   - Time patterns
   - Device relationships
   - Geographic patterns

2. **Risk Factors**
   - Report frequency
   - Activity type distribution
   - Account relationships
   - User feedback

3. **Edge Cases**
   - False positives
   - Legitimate high-frequency users
   - Regional variations
   - Platform differences

4. **Business Impact**
   - User safety
   - Revenue protection
   - Platform integrity
   - Legal compliance

## Follow-up Questions

1. How would you:
   - Handle false positives?
   - Scale the detection system?
   - Update risk scores?
   - Monitor effectiveness?

2. Consider:
   - New attack patterns
   - Legitimate edge cases
   - Regional differences
   - Platform changes

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_activities_composite 
ON activities(account_id, activity_date, activity_type);
CREATE INDEX idx_reports_composite 
ON reports(reported_id, report_date, report_type);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW daily_account_metrics AS
SELECT 
    account_id,
    DATE_TRUNC('day', activity_date) as day,
    COUNT(*) as activity_count,
    COUNT(DISTINCT activity_type) as unique_activities
FROM activities
GROUP BY account_id, DATE_TRUNC('day', activity_date);
```

3. **Query Optimization**
- Use appropriate partitioning
- Implement efficient joins
- Consider data sampling
- Create summary tables

---
# Survey Data 

## Table Structure
```sql
-- Table for survey events
Table: survey_events
event_id | user_id | survey_id | event_type | event_date         | response_value
--------------------------------------------------------------------------------
1001     | 101     | S1       | 'shown'    | 2024-01-01 10:00:00| NULL
1002     | 101     | S1       | 'response' | 2024-01-01 10:05:00| 4
1003     | 102     | S1       | 'shown'    | 2024-01-01 10:10:00| NULL
1004     | 103     | S1       | 'shown'    | 2024-01-01 10:15:00| NULL
1005     | 103     | S1       | 'response' | 2024-01-01 10:20:00| 5
1006     | 104     | S2       | 'shown'    | 2024-01-02 11:00:00| NULL
1007     | 105     | S2       | 'shown'    | 2024-01-02 11:05:00| NULL
1008     | 105     | S2       | 'response' | 2024-01-02 11:10:00| 3

-- Table for user information
Table: users
user_id | join_date  | country | user_type | last_active
--------------------------------------------------------------------------------
101     | 2023-01-01| 'US'    | 'new'     | 2024-01-01
102     | 2023-06-01| 'UK'    | 'existing'| 2024-01-01
103     | 2023-12-01| 'FR'    | 'new'     | 2024-01-01
104     | 2023-12-15| 'DE'    | 'new'     | 2024-01-02
105     | 2023-03-01| 'US'    | 'existing'| 2024-01-02

-- Table for survey metadata
Table: surveys
survey_id | survey_type | target_audience | launch_date | end_date
--------------------------------------------------------------------------------
S1       | 'product'   | 'all'          | 2024-01-01  | 2024-01-07
S2       | 'feature'   | 'new_users'    | 2024-01-02  | 2024-01-08
S3       | 'feedback'  | 'existing'     | 2024-01-03  | 2024-01-09
```

## Interview Questions and Solutions

### 1. Basic Response Rate Analysis

**Question:** Calculate the overall response rate for each survey.

**Clarification Points:**
- Definition of response rate (responses/impressions)
- Handling multiple responses
- Time window consideration
- User segment analysis

**Solution:**
```sql
WITH survey_metrics AS (
    SELECT 
        se.survey_id,
        s.survey_type,
        COUNT(DISTINCT CASE WHEN event_type = 'shown' THEN user_id END) as total_shown,
        COUNT(DISTINCT CASE WHEN event_type = 'response' THEN user_id END) as total_responses,
        AVG(CASE WHEN event_type = 'response' THEN response_value END) as avg_response_value
    FROM survey_events se
    JOIN surveys s ON se.survey_id = s.survey_id
    GROUP BY se.survey_id, s.survey_type
)
SELECT 
    survey_id,
    survey_type,
    total_shown,
    total_responses,
    ROUND(avg_response_value, 2) as avg_score,
    ROUND(
        total_responses * 100.0 / NULLIF(total_shown, 0),
        2
    ) as response_rate_percent
FROM survey_metrics
ORDER BY survey_id;
```

### 2. Response Rate by User Segment

**Question:** Compare survey response rates between new and existing users.

**Solution:**
```sql
WITH user_responses AS (
    SELECT 
        se.survey_id,
        u.user_type,
        COUNT(DISTINCT CASE WHEN event_type = 'shown' THEN se.user_id END) as impressions,
        COUNT(DISTINCT CASE WHEN event_type = 'response' THEN se.user_id END) as responses,
        AVG(CASE WHEN event_type = 'response' THEN response_value END) as avg_score
    FROM survey_events se
    JOIN users u ON se.user_id = u.user_id
    GROUP BY se.survey_id, u.user_type
)
SELECT 
    survey_id,
    user_type,
    impressions,
    responses,
    ROUND(avg_score, 2) as avg_response_value,
    ROUND(
        responses * 100.0 / NULLIF(impressions, 0),
        2
    ) as response_rate_percent,
    ROUND(
        responses * 100.0 / SUM(responses) OVER (PARTITION BY survey_id),
        2
    ) as segment_contribution_percent
FROM user_responses
ORDER BY survey_id, user_type;
```

### 3. Response Time Analysis

**Question:** Analyze how quickly users respond to surveys and the impact on response quality.

**Solution:**
```sql
WITH response_times AS (
    SELECT 
        se1.survey_id,
        se1.user_id,
        se1.event_date as shown_time,
        se2.event_date as response_time,
        se2.response_value,
        EXTRACT(EPOCH FROM (se2.event_date - se1.event_date))/60 as response_minutes
    FROM survey_events se1
    JOIN survey_events se2 
        ON se1.survey_id = se2.survey_id 
        AND se1.user_id = se2.user_id
        AND se1.event_type = 'shown'
        AND se2.event_type = 'response'
),
time_buckets AS (
    SELECT 
        survey_id,
        CASE 
            WHEN response_minutes <= 1 THEN 'immediate'
            WHEN response_minutes <= 5 THEN '1-5 mins'
            WHEN response_minutes <= 60 THEN '5-60 mins'
            ELSE 'over 1 hour'
        END as response_time_bucket,
        COUNT(*) as responses,
        AVG(response_value) as avg_score
    FROM response_times
    GROUP BY 
        survey_id,
        CASE 
            WHEN response_minutes <= 1 THEN 'immediate'
            WHEN response_minutes <= 5 THEN '1-5 mins'
            WHEN response_minutes <= 60 THEN '5-60 mins'
            ELSE 'over 1 hour'
        END
)
SELECT 
    survey_id,
    response_time_bucket,
    responses,
    ROUND(avg_score, 2) as avg_response_value,
    ROUND(
        responses * 100.0 / SUM(responses) OVER (PARTITION BY survey_id),
        2
    ) as percentage_of_responses
FROM time_buckets
ORDER BY 
    survey_id,
    CASE response_time_bucket
        WHEN 'immediate' THEN 1
        WHEN '1-5 mins' THEN 2
        WHEN '5-60 mins' THEN 3
        ELSE 4
    END;
```

### 4. Response Quality Analysis

**Question:** Analyze the relationship between user characteristics and response quality.

**Solution:**
```sql
WITH response_quality AS (
    SELECT 
        u.user_type,
        u.country,
        DATE_DIFF('day', u.join_date, se.event_date) as account_age_days,
        se.response_value,
        -- Calculate z-score for response values
        (se.response_value - AVG(se.response_value) OVER ()) /
        NULLIF(STDDEV(se.response_value) OVER (), 0) as response_zscore
    FROM survey_events se
    JOIN users u ON se.user_id = u.user_id
    WHERE se.event_type = 'response'
)
SELECT 
    user_type,
    country,
    ROUND(AVG(account_age_days), 0) as avg_account_age,
    COUNT(*) as response_count,
    ROUND(AVG(response_value), 2) as avg_response,
    ROUND(STDDEV(response_value), 2) as response_stddev,
    -- Calculate percentage of extreme responses
    ROUND(
        SUM(CASE WHEN ABS(response_zscore) > 2 THEN 1 ELSE 0 END) * 100.0 /
        COUNT(*),
        2
    ) as extreme_response_percent
FROM response_quality
GROUP BY user_type, country
HAVING COUNT(*) >= 5
ORDER BY response_count DESC;
```

## Key Considerations

1. **Response Rate Metrics**
   - Overall response rate
   - Segment-specific rates
   - Time-based patterns
   - Quality indicators

2. **User Segments**
   - New vs existing users
   - Geographic regions
   - User activity levels
   - Account age

3. **Edge Cases**
   - Multiple responses
   - Abandoned surveys
   - Invalid responses
   - Technical issues

4. **Business Impact**
   - Response quality
   - User satisfaction
   - Feature feedback
   - Product decisions

## Follow-up Questions

1. How would you:
   - Improve response rates?
   - Validate response quality?
   - Handle survey fatigue?
   - Target specific segments?

2. Consider:
   - Seasonal patterns
   - Cultural differences
   - Survey design
   - Response bias

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_survey_events_composite 
ON survey_events(survey_id, user_id, event_type, event_date);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW daily_survey_metrics AS
SELECT 
    survey_id,
    DATE_TRUNC('day', event_date) as day,
    COUNT(DISTINCT CASE WHEN event_type = 'shown' THEN user_id END) as daily_impressions,
    COUNT(DISTINCT CASE WHEN event_type = 'response' THEN user_id END) as daily_responses
FROM survey_events
GROUP BY survey_id, DATE_TRUNC('day', event_date);
```

3. **Partitioning Strategy**
```sql
-- Partition survey_events by date for better query performance
PARTITION BY RANGE (event_date);
```


---
# Friending

## Table Structure
```sql
-- Table for friend requests and actions
Table: friend_requests
ds          | request_id | timestamp | actor_id | receiver_id | action
--------------------------------------------------------------------------------
2024-01-01  | 1001      | 1704067200| 101      | 201        | 'request'
2024-01-01  | 1002      | 1704068100| 102      | 202        | 'request'
2024-01-01  | 1001      | 1704070800| 201      | 101        | 'accept'
2024-01-02  | 1003      | 1704154400| 103      | 203        | 'request'
2024-01-02  | 1002      | 1704158000| 202      | 102        | 'reject'
2024-01-03  | 1004      | 1704240800| 104      | 204        | 'request'
2024-01-03  | 1004      | 1704244400| 204      | 104        | 'accept'

-- Table for user demographics
Table: users
user_id | age_group | country | join_date  | active_status
--------------------------------------------------------------------------------
101     | '18-24'   | 'US'    | 2023-01-01| 'active'
102     | '25-34'   | 'UK'    | 2023-02-01| 'active'
103     | '18-24'   | 'FR'    | 2023-03-01| 'active'
104     | '35-44'   | 'DE'    | 2023-04-01| 'active'
201     | '25-34'   | 'US'    | 2023-05-01| 'active'
202     | '18-24'   | 'UK'    | 2023-06-01| 'inactive'
203     | '35-44'   | 'FR'    | 2023-07-01| 'active'
204     | '25-34'   | 'DE'    | 2023-08-01| 'active'
```

## Interview Questions and Solutions

### 1. Daily Acceptance Analysis for Past 4 Weeks

**Question:** Calculate the number of friend request acceptances for each day of the week over the past 4 weeks.

**Clarification Points:**
- Definition of acceptance (same request_id, action='accept')
- Day of week calculation
- 4-week window definition
- Handling time zones

**Solution:**
```sql
WITH daily_accepts AS (
    SELECT 
        ds,
        EXTRACT(DOW FROM DATE(ds)) as day_of_week,
        COUNT(*) as acceptance_count
    FROM friend_requests
    WHERE 
        action = 'accept'
        AND ds >= CURRENT_DATE - INTERVAL '28' DAY
        AND ds <= CURRENT_DATE
    GROUP BY ds, EXTRACT(DOW FROM DATE(ds))
)
SELECT 
    CASE day_of_week
        WHEN 0 THEN 'Sunday'
        WHEN 1 THEN 'Monday'
        WHEN 2 THEN 'Tuesday'
        WHEN 3 THEN 'Wednesday'
        WHEN 4 THEN 'Thursday'
        WHEN 5 THEN 'Friday'
        WHEN 6 THEN 'Saturday'
    END as weekday,
    COUNT(*) as total_days_with_data,
    SUM(acceptance_count) as total_acceptances,
    ROUND(AVG(acceptance_count), 2) as avg_acceptances_per_day,
    ROUND(STDDEV(acceptance_count), 2) as stddev_acceptances
FROM daily_accepts
GROUP BY day_of_week
ORDER BY day_of_week;
```

### 2. Friend Request Analysis by Age Group

**Question:** Calculate the average number of friendship requests sent per user over the past week by age group.

**Solution:**
```sql
WITH request_counts AS (
    SELECT 
        u.age_group,
        fr.actor_id,
        COUNT(*) as requests_sent
    FROM friend_requests fr
    JOIN users u ON fr.actor_id = u.user_id
    WHERE 
        fr.action = 'request'
        AND fr.ds >= CURRENT_DATE - INTERVAL '7' DAY
    GROUP BY u.age_group, fr.actor_id
),
age_group_stats AS (
    SELECT 
        u.age_group,
        COUNT(DISTINCT u.user_id) as total_users
    FROM users u
    WHERE u.active_status = 'active'
    GROUP BY u.age_group
)
SELECT 
    rc.age_group,
    ags.total_users,
    COUNT(DISTINCT rc.actor_id) as users_sending_requests,
    ROUND(
        COUNT(DISTINCT rc.actor_id) * 100.0 / 
        NULLIF(ags.total_users, 0),
        2
    ) as pct_users_sending_requests,
    ROUND(AVG(rc.requests_sent), 2) as avg_requests_per_sending_user,
    ROUND(
        SUM(rc.requests_sent) * 1.0 / 
        NULLIF(ags.total_users, 0),
        2
    ) as avg_requests_per_total_users
FROM request_counts rc
JOIN age_group_stats ags ON rc.age_group = ags.age_group
GROUP BY rc.age_group, ags.total_users
ORDER BY avg_requests_per_sending_user DESC;
```

### 3. Friend Request Success Rate Analysis

**Question:** Analyze friend request success rates by user demographics and patterns.

**Solution:**
```sql
WITH request_outcomes AS (
    SELECT 
        fr1.request_id,
        fr1.actor_id,
        fr1.receiver_id,
        u1.age_group as sender_age_group,
        u2.age_group as receiver_age_group,
        u1.country as sender_country,
        u2.country as receiver_country,
        COALESCE(fr2.action, 'pending') as outcome,
        CASE 
            WHEN fr2.action IS NOT NULL 
            THEN EXTRACT(EPOCH FROM (
                TO_TIMESTAMP(fr2.timestamp) - 
                TO_TIMESTAMP(fr1.timestamp)
            ))/3600.0 
            ELSE NULL 
        END as response_time_hours
    FROM friend_requests fr1
    LEFT JOIN friend_requests fr2 
        ON fr1.request_id = fr2.request_id
        AND fr2.action IN ('accept', 'reject')
    JOIN users u1 ON fr1.actor_id = u1.user_id
    JOIN users u2 ON fr1.receiver_id = u2.user_id
    WHERE 
        fr1.action = 'request'
        AND fr1.ds >= CURRENT_DATE - INTERVAL '30' DAY
)
SELECT 
    sender_age_group,
    receiver_age_group,
    COUNT(*) as total_requests,
    COUNT(CASE WHEN outcome = 'accept' THEN 1 END) as acceptances,
    COUNT(CASE WHEN outcome = 'reject' THEN 1 END) as rejections,
    COUNT(CASE WHEN outcome = 'pending' THEN 1 END) as pending,
    ROUND(
        COUNT(CASE WHEN outcome = 'accept' THEN 1 END) * 100.0 / 
        NULLIF(COUNT(CASE WHEN outcome != 'pending' THEN 1 END), 0),
        2
    ) as acceptance_rate,
    ROUND(AVG(CASE WHEN outcome = 'accept' THEN response_time_hours END), 2) as avg_acceptance_time_hours
FROM request_outcomes
GROUP BY sender_age_group, receiver_age_group
HAVING COUNT(*) >= 10
ORDER BY total_requests DESC;
```

### 4. Network Growth Analysis

**Question:** Analyze the growth of friendship networks over time.

**Solution:**
```sql
WITH daily_metrics AS (
    SELECT 
        ds,
        COUNT(DISTINCT CASE WHEN action = 'request' THEN actor_id END) as unique_senders,
        COUNT(DISTINCT CASE WHEN action = 'request' THEN receiver_id END) as unique_receivers,
        COUNT(DISTINCT CASE WHEN action = 'accept' THEN request_id END) as new_connections,
        COUNT(DISTINCT CASE WHEN action = 'request' THEN request_id END) as total_requests
    FROM friend_requests
    WHERE ds >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY ds
),
rolling_metrics AS (
    SELECT 
        ds,
        unique_senders,
        unique_receivers,
        new_connections,
        total_requests,
        AVG(new_connections) OVER (
            ORDER BY ds 
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
        ) as rolling_7day_connections,
        SUM(new_connections) OVER (ORDER BY ds) as cumulative_connections
    FROM daily_metrics
)
SELECT 
    ds,
    unique_senders,
    unique_receivers,
    new_connections,
    ROUND(
        new_connections * 100.0 / 
        NULLIF(total_requests, 0),
        2
    ) as daily_acceptance_rate,
    ROUND(rolling_7day_connections, 2) as avg_7day_connections,
    cumulative_connections
FROM rolling_metrics
ORDER BY ds;
```

## Key Considerations

1. **Temporal Patterns**
   - Day of week effects
   - Time zone handling
   - Response time analysis
   - Seasonal patterns

2. **User Demographics**
   - Age group analysis
   - Geographic patterns
   - User activity levels
   - Network effects

3. **Edge Cases**
   - Pending requests
   - Multiple requests
   - Inactive users
   - Cross-region requests

4. **Business Impact**
   - Network growth
   - User engagement
   - Community health
   - Platform stickiness

## Common Follow-up Questions

1. How would you:
   - Handle repeat requests?
   - Measure network quality?
   - Identify power users?
   - Track network density?

2. Consider:
   - Cultural differences
   - Platform features
   - Privacy settings
   - User preferences

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_requests_composite 
ON friend_requests(ds, action, actor_id, receiver_id);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW daily_friendship_metrics AS
SELECT 
    ds,
    COUNT(DISTINCT CASE WHEN action = 'request' THEN request_id END) as daily_requests,
    COUNT(DISTINCT CASE WHEN action = 'accept' THEN request_id END) as daily_accepts
FROM friend_requests
GROUP BY ds;
```

3. **Partitioning Strategy**
```sql
-- Partition by date for better query performance
PARTITION BY RANGE (ds);
```
---
# Session Analysis - Meta SQL Interview Guide

## Table Structure
```sql
-- Table for session events
Table: session_events
event_id | user_id | session_id | page_id | event_type | timestamp          | duration_sec
------------------------------------------------------------------------------------------
1001     | 101     | S1001     | 'A'     | 'view'    | 2024-01-01 10:00:00| 120
1002     | 101     | S1001     | 'B'     | 'view'    | 2024-01-01 10:02:00| 60
1003     | 101     | S1001     | 'A'     | 'view'    | 2024-01-01 10:03:00| 30
1004     | 102     | S1002     | 'A'     | 'view'    | 2024-01-01 10:05:00| 300
1005     | 102     | S1002     | 'C'     | 'view'    | 2024-01-01 10:10:00| 180
1006     | 101     | S1003     | 'B'     | 'view'    | 2024-01-01 11:00:00| 240
1007     | 103     | S1004     | 'A'     | 'view'    | 2024-01-01 11:05:00| 60
1008     | 103     | S1004     | 'B'     | 'view'    | 2024-01-01 11:06:00| 120

-- Table for user information
Table: users
user_id | country | device_type | join_date
--------------------------------------------------------------------------------
101     | 'US'    | 'mobile'   | 2023-01-01
102     | 'UK'    | 'desktop'  | 2023-02-01
103     | 'FR'    | 'mobile'   | 2023-03-01
104     | 'DE'    | 'tablet'   | 2023-04-01
```

## Interview Questions and Solutions

### 1. Time Between Sessions Analysis

**Question:** Calculate the time elapsed between consecutive sessions for each user.

**Clarification Points:**
- Session definition
- Time window for analysis
- Handling first sessions
- Cross-day sessions

**Solution:**
```sql
WITH session_times AS (
    -- Get first event of each session
    SELECT 
        user_id,
        session_id,
        MIN(timestamp) as session_start,
        MAX(timestamp) as session_end,
        SUM(duration_sec) as total_duration
    FROM session_events
    GROUP BY user_id, session_id
),
session_gaps AS (
    SELECT 
        user_id,
        session_id,
        session_start,
        session_end,
        total_duration,
        -- Calculate time to next session
        LEAD(session_start) OVER (
            PARTITION BY user_id 
            ORDER BY session_start
        ) as next_session_start,
        -- Calculate gap to next session in minutes
        EXTRACT(EPOCH FROM (
            LEAD(session_start) OVER (
                PARTITION BY user_id 
                ORDER BY session_start
            ) - session_end
        ))/60 as gap_minutes
    FROM session_times
)
SELECT 
    user_id,
    COUNT(*) as total_sessions,
    ROUND(AVG(gap_minutes), 2) as avg_gap_minutes,
    ROUND(MIN(gap_minutes), 2) as min_gap_minutes,
    ROUND(MAX(gap_minutes), 2) as max_gap_minutes,
    -- Categorize gaps
    SUM(CASE 
        WHEN gap_minutes <= 30 THEN 1 
        ELSE 0 
    END) as quick_returns,
    SUM(CASE 
        WHEN gap_minutes > 30 AND gap_minutes <= 1440 THEN 1 
        ELSE 0 
    END) as same_day_returns,
    SUM(CASE 
        WHEN gap_minutes > 1440 THEN 1 
        ELSE 0 
    END) as next_day_plus_returns
FROM session_gaps
WHERE gap_minutes IS NOT NULL
GROUP BY user_id
ORDER BY total_sessions DESC;
```

### 2. Bounce Rate Analysis (A->B->A Pattern)

**Question:** Calculate the bounce rate where a bounce is defined as a user returning to page A after visiting page B.

**Solution:**
```sql
WITH page_sequences AS (
    SELECT 
        user_id,
        session_id,
        page_id,
        timestamp,
        LEAD(page_id, 1) OVER (
            PARTITION BY user_id, session_id 
            ORDER BY timestamp
        ) as next_page,
        LEAD(page_id, 2) OVER (
            PARTITION BY user_id, session_id 
            ORDER BY timestamp
        ) as next_next_page
    FROM session_events
),
bounce_patterns AS (
    SELECT 
        user_id,
        session_id,
        COUNT(*) as total_patterns,
        SUM(CASE 
            WHEN page_id = 'A' 
            AND next_page = 'B' 
            AND next_next_page = 'A' 
            THEN 1 
            ELSE 0 
        END) as bounce_patterns
    FROM page_sequences
    GROUP BY user_id, session_id
)
SELECT 
    COUNT(DISTINCT session_id) as total_sessions,
    SUM(bounce_patterns) as total_bounces,
    ROUND(
        SUM(bounce_patterns) * 100.0 / 
        COUNT(DISTINCT session_id),
        2
    ) as bounce_rate_percent
FROM bounce_patterns;
```

### 3. Detailed Bounce Analysis by User Segments

**Question:** Analyze bounce patterns across different user segments and device types.

**Solution:**
```sql
WITH page_transitions AS (
    SELECT 
        se.user_id,
        se.session_id,
        u.country,
        u.device_type,
        se.page_id,
        se.timestamp,
        LEAD(se.page_id) OVER (
            PARTITION BY se.user_id, se.session_id 
            ORDER BY se.timestamp
        ) as next_page,
        LEAD(se.page_id, 2) OVER (
            PARTITION BY se.user_id, se.session_id 
            ORDER BY se.timestamp
        ) as next_next_page
    FROM session_events se
    JOIN users u ON se.user_id = u.user_id
),
segment_bounces AS (
    SELECT 
        country,
        device_type,
        COUNT(DISTINCT session_id) as total_sessions,
        COUNT(DISTINCT CASE 
            WHEN page_id = 'A' 
            AND next_page = 'B' 
            AND next_next_page = 'A' 
            THEN session_id 
        END) as bounce_sessions
    FROM page_transitions
    GROUP BY country, device_type
)
SELECT 
    country,
    device_type,
    total_sessions,
    bounce_sessions,
    ROUND(
        bounce_sessions * 100.0 / 
        NULLIF(total_sessions, 0),
        2
    ) as bounce_rate_percent
FROM segment_bounces
ORDER BY total_sessions DESC;
```

### 4. Session Quality Metrics

**Question:** Calculate comprehensive session quality metrics including depth and engagement.

**Solution:**
```sql
WITH session_metrics AS (
    SELECT 
        se.session_id,
        se.user_id,
        u.device_type,
        COUNT(DISTINCT se.page_id) as unique_pages,
        COUNT(*) as total_events,
        SUM(se.duration_sec) as total_duration,
        MAX(timestamp) - MIN(timestamp) as session_length,
        -- Calculate if it's a bounce (A->B->A pattern)
        MAX(CASE 
            WHEN se.page_id = 'A' 
            AND LEAD(se.page_id) OVER (
                PARTITION BY se.session_id 
                ORDER BY se.timestamp
            ) = 'B'
            AND LEAD(se.page_id, 2) OVER (
                PARTITION BY se.session_id 
                ORDER BY se.timestamp
            ) = 'A'
            THEN 1 
            ELSE 0 
        END) as is_bounce
    FROM session_events se
    JOIN users u ON se.user_id = u.user_id
    GROUP BY se.session_id, se.user_id, u.device_type
)
SELECT 
    device_type,
    COUNT(*) as total_sessions,
    ROUND(AVG(unique_pages), 2) as avg_pages_per_session,
    ROUND(AVG(total_events), 2) as avg_events_per_session,
    ROUND(AVG(EXTRACT(EPOCH FROM session_length)), 2) as avg_session_seconds,
    ROUND(AVG(total_duration), 2) as avg_active_seconds,
    ROUND(
        SUM(is_bounce) * 100.0 / 
        COUNT(*),
        2
    ) as bounce_rate_percent
FROM session_metrics
GROUP BY device_type
ORDER BY total_sessions DESC;
```

## Key Considerations

1. **Time Calculations**
   - Session boundaries
   - Timezone handling
   - Inactive periods
   - Cross-day sessions

2. **Bounce Patterns**
   - Pattern definition
   - Sequential navigation
   - Time thresholds
   - User intent

3. **Edge Cases**
   - Single-page sessions
   - Technical issues
   - Multiple devices
   - Parallel sessions

4. **Business Impact**
   - User engagement
   - Content effectiveness
   - Navigation patterns
   - Technical issues

## Follow-up Questions

1. How would you:
   - Handle timezone differences?
   - Define session boundaries?
   - Measure quality engagement?
   - Account for page types?

2. Consider:
   - Mobile vs desktop
   - New vs returning users
   - Feature releases
   - A/B tests

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
CREATE INDEX idx_sessions_composite 
ON session_events(user_id, session_id, timestamp);
```

2. **Materialized Views**
```sql
CREATE MATERIALIZED VIEW session_summaries AS
SELECT 
    user_id,
    session_id,
    COUNT(*) as events,
    MIN(timestamp) as start_time,
    MAX(timestamp) as end_time
FROM session_events
GROUP BY user_id, session_id;
```

3. **Partitioning Strategy**
```sql
-- Partition by date range for better query performance
PARTITION BY RANGE (timestamp);
```
---

# User Behavior Analysis - Meta SQL Interview Guide

## 1. Phone Number Confirmation Analysis

### Tables
```sql
-- Table for phone confirmations
Table: phone_confirmations
confirmation_id | phone_number | send_date           | confirmation_type | status
--------------------------------------------------------------------------------
1001           | '1234567890' | 2024-01-01 10:00:00| 'SMS'            | 'sent'
1002           | '2345678901' | 2024-01-01 10:05:00| 'SMS'            | 'sent'
1003           | '1234567890' | 2024-01-01 10:10:00| 'SMS'            | 'confirmed'
1004           | '3456789012' | 2024-01-01 11:00:00| 'WhatsApp'       | 'sent'
1005           | '2345678901' | 2024-01-01 11:05:00| 'SMS'            | 'failed'

-- Table for user accounts
Table: user_accounts
user_id | phone_number | signup_date | country | verification_status
--------------------------------------------------------------------------------
101     | '1234567890' | 2024-01-01 | 'US'    | 'verified'
102     | '2345678901' | 2024-01-01 | 'UK'    | 'pending'
103     | '3456789012' | 2024-01-01 | 'FR'    | 'pending'
104     | '4567890123' | 2024-01-01 | 'DE'    | 'unverified'
```

### Questions & Solutions

1. **Calculate Confirmation Success Rate**
```sql
WITH confirmation_attempts AS (
    SELECT 
        phone_number,
        confirmation_type,
        COUNT(*) as total_attempts,
        COUNT(CASE WHEN status = 'confirmed' THEN 1 END) as successful_confirms
    FROM phone_confirmations
    WHERE send_date >= CURRENT_DATE - INTERVAL '30' DAY
    GROUP BY phone_number, confirmation_type
)
SELECT 
    confirmation_type,
    COUNT(DISTINCT phone_number) as unique_numbers,
    SUM(total_attempts) as total_attempts,
    SUM(successful_confirms) as total_confirms,
    ROUND(
        SUM(successful_confirms) * 100.0 / 
        NULLIF(SUM(total_attempts), 0),
        2
    ) as success_rate_percent
FROM confirmation_attempts
GROUP BY confirmation_type
ORDER BY success_rate_percent DESC;
```

## 2. App Switching Analysis

### Tables
```sql
-- Table for app usage events
Table: app_events
event_id | user_id | app_name   | event_type | timestamp
--------------------------------------------------------------------------------
1001     | 101     | 'Facebook' | 'open'     | 2024-01-01 10:00:00
1002     | 101     | 'Messenger'| 'open'     | 2024-01-01 10:02:00
1003     | 101     | 'Facebook' | 'open'     | 2024-01-01 10:05:00
1004     | 102     | 'Facebook' | 'open'     | 2024-01-01 10:10:00
1005     | 102     | 'Messenger'| 'open'     | 2024-01-01 10:15:00
1006     | 103     | 'Facebook' | 'open'     | 2024-01-01 11:00:00
```

### Questions & Solutions

1. **Identify FB -> Messenger -> FB Pattern**
```sql
WITH app_sequences AS (
    SELECT 
        user_id,
        app_name,
        timestamp,
        LEAD(app_name, 1) OVER (
            PARTITION BY user_id 
            ORDER BY timestamp
        ) as next_app,
        LEAD(app_name, 2) OVER (
            PARTITION BY user_id 
            ORDER BY timestamp
        ) as next_next_app,
        -- Calculate time differences
        EXTRACT(EPOCH FROM (
            LEAD(timestamp, 1) OVER (
                PARTITION BY user_id 
                ORDER BY timestamp
            ) - timestamp
        )) as time_to_next_sec
    FROM app_events
    WHERE event_type = 'open'
),
switching_patterns AS (
    SELECT 
        user_id,
        COUNT(*) as total_switches,
        SUM(CASE 
            WHEN app_name = 'Facebook' 
            AND next_app = 'Messenger' 
            AND next_next_app = 'Facebook' 
            AND time_to_next_sec <= 300  -- Within 5 minutes
            THEN 1 
            ELSE 0 
        END) as fb_messenger_fb_switches
    FROM app_sequences
    GROUP BY user_id
    HAVING SUM(CASE 
        WHEN app_name = 'Facebook' 
        AND next_app = 'Messenger' 
        AND next_next_app = 'Facebook' 
        AND time_to_next_sec <= 300
        THEN 1 
        ELSE 0 
    END) > 0
)
SELECT 
    user_id,
    total_switches,
    fb_messenger_fb_switches,
    ROUND(
        fb_messenger_fb_switches * 100.0 / 
        NULLIF(total_switches, 0),
        2
    ) as switch_pattern_percent
FROM switching_patterns
ORDER BY fb_messenger_fb_switches DESC;
```

## 3. User Status Tracking

### Tables
```sql
-- Table for user status events
Table: status_events
event_id | user_id | status    | timestamp
--------------------------------------------------------------------------------
1001     | 101     | 'active'  | 2024-01-01 10:00:00
1002     | 102     | 'active'  | 2024-01-01 10:05:00
1003     | 101     | 'away'    | 2024-01-01 10:10:00
1004     | 103     | 'active'  | 2024-01-01 11:00:00
1005     | 101     | 'offline' | 2024-01-01 12:00:00

-- Table for daily status snapshot
Table: daily_status
date       | user_id | last_status | last_update_time
--------------------------------------------------------------------------------
2024-01-01 | 101     | 'offline'   | 2024-01-01 12:00:00
2024-01-01 | 102     | 'active'    | 2024-01-01 10:05:00
2024-01-01 | 103     | 'active'    | 2024-01-01 11:00:00
```

### Questions & Solutions

1. **Create Daily Status Tracking Table**
```sql
-- First, create the table
CREATE TABLE daily_status (
    date DATE,
    user_id BIGINT,
    last_status VARCHAR(50),
    last_update_time TIMESTAMP,
    PRIMARY KEY (date, user_id)
);

-- Then, populate it with the latest status for each user per day
WITH latest_status AS (
    SELECT 
        DATE(timestamp) as status_date,
        user_id,
        status,
        timestamp as update_time,
        ROW_NUMBER() OVER (
            PARTITION BY user_id, DATE(timestamp) 
            ORDER BY timestamp DESC
        ) as rn
    FROM status_events
)
INSERT INTO daily_status 
SELECT 
    status_date,
    user_id,
    status as last_status,
    update_time as last_update_time
FROM latest_status
WHERE rn = 1;
```

2. **Update Daily Status with New Events**
```sql
-- Create procedure to update daily status
CREATE OR REPLACE PROCEDURE update_daily_status()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Insert new records for today
    INSERT INTO daily_status (date, user_id, last_status, last_update_time)
    SELECT DISTINCT
        CURRENT_DATE,
        se.user_id,
        LAST_VALUE(se.status) OVER (
            PARTITION BY se.user_id 
            ORDER BY se.timestamp 
            ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
        ) as last_status,
        MAX(se.timestamp) OVER (
            PARTITION BY se.user_id
        ) as last_update_time
    FROM status_events se
    WHERE DATE(se.timestamp) = CURRENT_DATE
    ON CONFLICT (date, user_id) 
    DO UPDATE SET
        last_status = EXCLUDED.last_status,
        last_update_time = EXCLUDED.last_update_time
    WHERE daily_status.last_update_time < EXCLUDED.last_update_time;

    -- Carry over last known status for users with no events today
    INSERT INTO daily_status (date, user_id, last_status, last_update_time)
    SELECT 
        CURRENT_DATE,
        ds.user_id,
        ds.last_status,
        ds.last_update_time
    FROM daily_status ds
    WHERE ds.date = CURRENT_DATE - INTERVAL '1' DAY
    AND NOT EXISTS (
        SELECT 1 
        FROM daily_status today 
        WHERE today.date = CURRENT_DATE 
        AND today.user_id = ds.user_id
    );
END;
$$;
```

## Key Considerations

1. **Phone Confirmation Analysis**
   - Handling multiple attempts
   - Different confirmation methods
   - Timeout periods
   - Regional patterns

2. **App Switching Analysis**
   - Time window definition
   - Session boundaries
   - Multiple devices
   - App state handling

3. **Status Tracking**
   - Status persistence
   - Data consistency
   - Historical tracking
   - Performance optimization

## Follow-up Questions

1. How would you:
   - Handle timezone differences?
   - Measure confirmation effectiveness?
   - Track cross-device patterns?
   - Optimize storage?

2. Consider:
   - Data retention policies
   - Privacy requirements
   - Scale considerations
   - Real-time requirements

## Performance Optimization Tips

1. **Indexing Strategy**
```sql
-- For phone confirmations
CREATE INDEX idx_phone_confirms ON phone_confirmations(phone_number, send_date);

-- For app events
CREATE INDEX idx_app_events ON app_events(user_id, timestamp);

-- For status tracking
CREATE INDEX idx_status_events ON status_events(user_id, timestamp);
```

2. **Partitioning Strategy**
```sql
-- Partition status tracking by date
PARTITION BY RANGE (date);
```

3. **Materialized Views**
```sql
-- Create summary views for frequently accessed patterns
CREATE MATERIALIZED VIEW daily_user_patterns AS
SELECT 
    user_id,
    DATE(timestamp) as event_date,
    COUNT(*) as total_events,
    COUNT(DISTINCT app_name) as unique_apps
FROM app_events
GROUP BY user_id, DATE(timestamp);
```

[2024-12-10 Meta DSA VO 面经](https://www.1point3acres.com/bbs/thread-1102145-1-1.html)  
shop visibility，好像在地里出现过，会follow up产品问题，难度不大  
[2024-11-28 买它 VO 过经](https://www.1point3acres.com/bbs/thread-1100228-1-1.html)  

[2024-02-22 Meta DSA VO](https://www.1point3acres.com/bbs/thread-1047008-1-1.html)  
sql轮的题目没在地里看过，问的是找global ads revenue in a year, 第二题要求计算global rolling revenue in us dollar (划重点：要用给的foreign exchange rate table把别的国家的revenue换成us dollar, 然后要会rolling sum），第三题问找出第一次超过1 million revenue的那一天。题目考完了还问了ads和revenue 的trade off（说好的sql轮呢。。。。）所以大家要做好准备sql轮也可能考case！

[2024-11-23 买它新鲜跪经 - DS - VO](https://www.1point3acres.com/bbs/thread-1099426-1-1.html)  
不确定地里有没有这SQL题。关于店铺的是否对客户可见。题设是，店家的店铺可以随时切换店铺是否对客户可见。  
[2024-4-15 Meta DSA VO](https://www.1point3acres.com/bbs/thread-1061408-1-1.html)  
广告在不同国家投放，每投放一次产生revenue. 算总体revenue，要注意convert成US currency  
[2024-11-20 买它 VO 新鲜出炉 细节+讨论+等结果](https://www.1point3acres.com/bbs/thread-1098942-1-1.html)  
题都是一样的。最后让定义unhealthy user. 我一开始定义的不全面，有几次和面试官的back-and-forth，但是也不知道最后的想法如何。草草写完final version+解释完就结束了。欢迎补充这题大家怎么定义unhealthy的。

[2024-11-15]